<rss xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html" version="2.0">
    <channel>
        <title>
            <![CDATA[ Towards Data Science - Medium ]]>
        </title>
        <description>
            <![CDATA[ Your home for data science. A Medium publication sharing concepts, ideas and codes. - Medium ]]>
        </description>
        <link>https://towardsdatascience.com?source=rss----7f60cf5620c9---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Towards Data Science - Medium</title>
            <link>https://towardsdatascience.com?source=rss----7f60cf5620c9---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 11 Apr 2024 12:10:50 GMT</lastBuildDate>
        <atom:link href="https://towardsdatascience.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster>
            <![CDATA[ yourfriends@medium.com ]]>
        </webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title>
                <![CDATA[ Using a Multimodal Document ML Model to Query Your Documents ]]>
            </title>
            <description>
                <![CDATA[ <div class="medium-feed-item"><p class="medium-feed-image"><a href="https://towardsdatascience.com/using-a-multimodal-document-ml-model-to-query-your-documents-9bb3a2199380?source=rss----7f60cf5620c9---4"><img src="https://cdn-images-1.medium.com/max/1792/0*ddD762A_JsOSp6uL" width="1792"></a></p><p class="medium-feed-snippet">Leverage the power of the mPLUG-Owl document understanding model to ask questions about your documents</p><p class="medium-feed-link"><a href="https://towardsdatascience.com/using-a-multimodal-document-ml-model-to-query-your-documents-9bb3a2199380?source=rss----7f60cf5620c9---4">Continue reading on Towards Data Science »</a></p></div> ]]>
            </description>
            <link>https://towardsdatascience.com/using-a-multimodal-document-ml-model-to-query-your-documents-9bb3a2199380?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/9bb3a2199380</guid>
            <category>
                <![CDATA[ document-management ]]>
            </category>
            <category>
                <![CDATA[ ai ]]>
            </category>
            <category>
                <![CDATA[ machine-learning ]]>
            </category>
            <category>
                <![CDATA[ document-understanding ]]>
            </category>
            <category>
                <![CDATA[ data-science ]]>
            </category>
            <dc:creator>
                <![CDATA[ Eivind Kjosbakken ]]>
            </dc:creator>
            <pubDate>Thu, 11 Apr 2024 05:17:17 GMT</pubDate>
            <atom:updated>2024-04-11T05:17:16.819Z</atom:updated>
        </item>
        <item>
            <title>
                <![CDATA[ Moirai: Time Series Foundation Models for Universal Forecasting ]]>
            </title>
            <description>
                <![CDATA[ <div class="medium-feed-item"><p class="medium-feed-image"><a href="https://towardsdatascience.com/moirai-time-series-foundation-models-for-universal-forecasting-dc93f74b330f?source=rss----7f60cf5620c9---4"><img src="https://cdn-images-1.medium.com/max/2000/1*PhqFhEKc8mA-gxWa8Z6oDw.png" width="2000"></a></p><p class="medium-feed-snippet">The future of predictive analytics: Explore Moirai, Salesforce&#x2019;s new foundation model for advanced time series forecasting</p><p class="medium-feed-link"><a href="https://towardsdatascience.com/moirai-time-series-foundation-models-for-universal-forecasting-dc93f74b330f?source=rss----7f60cf5620c9---4">Continue reading on Towards Data Science »</a></p></div> ]]>
            </description>
            <link>https://towardsdatascience.com/moirai-time-series-foundation-models-for-universal-forecasting-dc93f74b330f?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/dc93f74b330f</guid>
            <category>
                <![CDATA[ time-series-forecasting ]]>
            </category>
            <category>
                <![CDATA[ data-science ]]>
            </category>
            <category>
                <![CDATA[ python ]]>
            </category>
            <category>
                <![CDATA[ machine-learning ]]>
            </category>
            <category>
                <![CDATA[ thoughts-and-theory ]]>
            </category>
            <dc:creator>
                <![CDATA[ Luís Roque ]]>
            </dc:creator>
            <pubDate>Thu, 11 Apr 2024 05:05:21 GMT</pubDate>
            <atom:updated>2024-04-11T05:05:21.560Z</atom:updated>
        </item>
        <item>
            <title>
                <![CDATA[ How to make the most out of LLM production data: simulated user feedback ]]>
            </title>
            <link>https://towardsdatascience.com/how-to-make-the-most-out-of-llm-production-data-simulated-user-feedback-843c444febc7?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/843c444febc7</guid>
            <category>
                <![CDATA[ llm ]]>
            </category>
            <category>
                <![CDATA[ genai ]]>
            </category>
            <category>
                <![CDATA[ simulation ]]>
            </category>
            <category>
                <![CDATA[ llmops ]]>
            </category>
            <category>
                <![CDATA[ llm-evaluation ]]>
            </category>
            <dc:creator>
                <![CDATA[ Pasquale Antonante, Ph.D. ]]>
            </dc:creator>
            <pubDate>Thu, 11 Apr 2024 03:39:15 GMT</pubDate>
            <atom:updated>2024-04-11T03:39:15.044Z</atom:updated>
            <content:encoded>
                <![CDATA[ <h3>How to Make the Most Out of LLM Production Data: Simulated User Feedback</h3><h4>A novel approach to use production data to simulate user feedback for testing and evaluating your LLM app</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*r3Ov2EIMg_xfZJ1vavTbPA.jpeg" /><figcaption>Image by author and ChatGPT. “Image of two llamas, one with a thumbs up and another with thumbs down” prompt. ChatGPT, 4, OpenAI, April 10th 2024. <a href="https://chat.openai.com./">https://chat.openai.com.</a></figcaption></figure><p>A series of blog posts to share our perspectives on how to evaluate and improve your GenAI application pipelines</p><p><em>(written by Pasquale Antonante and Yi Zhang at Relari.ai)</em></p><p>The world of LLM app development is always on the move — new tricks, models, and apps pop up every week. As tech gets better, what users expect keeps ramping up. Staying ahead in this game is key to making sure it’s the one users keep coming back to.</p><p>The problem now becomes: how do you measure performance improvements? When you’re fiddling with prompts, tweaking the temperature, or switching up models, do you ever pause and think, “Will my users actually like this more?</p><p>In this post, we’ll walk through how in-app user feedback from earlier deployments (or internal human evaluation) can be instrumental in quickly shaping future versions of a product. We’ll discuss the limitations of traditional feedback mechanisms and introduce a new technique that allows AI developers to use feedback data directly in offline testing and iterations (before a new deployment), making the development cycle more adaptable and responsive to user preferences.</p><h3>Understanding the Value of User Feedback</h3><h4>Why User Feedback Matters and Its Challenges</h4><p>When developing LLM-based applications, we are often faced with a particular problem we want to address,<em> e.g.</em>, a specific type of question has a low accuracy. As we experiment with tweaks in prompts, parameters, architecture, etc., we want to evaluate performance of the new pipeline, in particular whether users will like the new version(s) of your AI application. The most straightforward way is to A/B test each change with the end users and collect their feedback data such as thumbs up / down, score rating, or written comments, but practically it is challenging for a few reasons:</p><ol><li><strong>Slow to collect: </strong>Unless your application is already seeing huge volume, you don’t have that much feedback data. Anecdotally, we’ve seen the feedback participation rate in our customers’ AI applications range from &lt;1% (normal) to ~10% (exceptional, often through deliberate UI/UX design to encourage more feedback). As a result, it can take a long time before you get enough feedback data to make a statistically confident judgment of whether a particular change resonated positively or negatively with your users.</li><li><strong>Risk of jeopardizing user relationships: </strong>Testing directly with users is the most effective way to gain insights, but there’s a real risk of damaging your relationship with them if they encounter an unsatisfactory version. Users can be quick to judge, potentially dismissing your application at the first sign of a mistake. Consequently, developers tend to opt for more conservative or less disruptive changes for A/B testing with users, reserving the bolder, more innovative updates for internal testing. This approach allows for experimentation while minimizing the risk of alienating the user base.</li><li><strong>Inconsistent measurement: </strong>With most AI applications being fairly open-ended, it is often difficult to get truly apples-to-apples comparison of feedback data given different users can interact with your product in a different way. As a result, feedback data A/B testing for LLM-based applications tends to be more noisy than those from traditional applications.</li></ol><p>In the next section, we’ll introduce a novel approach that we’ve deployed to multiple customers to help them make the most out of their user feedback data in offline development.</p><h3>A Novel Approach: Simulate User Feedback</h3><p>In response to these challenges in collecting user feedback, we have developed a novel approach to simulate user feedback using a small sample of user (or internally labeled) feedback data. Specifically, we use metric ensembling and conformal prediction to learn user preferences and use them offline during the development phase. At its core, we learn how users weigh different criteria (e.g., tone, conciseness, etc) and leverage conformal prediction to provide predictions to quantify confidence. This method drastically accelerates LLM app development by providing a way to anticipate how users might react to new features or changes before they are fully implemented.</p><p>To evaluate its effectiveness, we compared this approach with the more conventional one of using a single LLM call that assesses different aspects of the response to make a judgment. To compare the two alternatives (the proposed approach vs. the single LLM call), we conducted an experiment using the <a href="https://huggingface.co/datasets/llm-blender/Unified-Feedback">Unified-Feedback</a> dataset. We generated many potential …..We used Kendall’s tau, a measure of rank correlation, to compare the rankings produced by our user feedback simulation and the single LLM call approach against the ground truth established by human evaluations. This analysis allows us to assess not only the degree of agreement, but also the order of preference that each method predicts compared to the human rankings.</p><p><strong>Our experiment revealed that the user feedback simulation has a correlation of 93% that significantly exceeded that of the single LLM call approach, which attains roughly 70% correlation. This indicates that, in terms of ranking , the simulated user feedback simulation provides a closer approximation to human judgment.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*o2lp3jGAJWg-T7T_" /><figcaption>Kendall’s tau of the two methods. Higher values indicate a stronger correlation between the ranking produced by the method and the human. Simulated User Feedback (proposed, lighter blue) shows higher agreement with humans when tasked with identifying improvements, suggesting that it more accurately reflects human judgment in identifying and evaluating improvements. Image by the author.</figcaption></figure><p>The reason why the simulated user feedback performs better is twofold:</p><ol><li>it learns from actual user feedback the importance of different criteria, making the approach custom to your use case</li><li>while individual criteria may have appeared in the LLM training set, the complex (and potentially large) set of different criteria likely have not appeared in the training data, making it more difficult for the LLM evaluator to get right.</li></ol><p>While single LLM calls can identify major improvements in the pipeline, they fall short of detecting the more frequent, minor enhancements critical in mature pipelines. Simulated user feedback, however, exhibits a high correlation with human judgment, enabling the detection of these incremental advances.</p><p>As a side note, while we could have used the data to fine-tune an LLM, this has the typical drawback of requiring more data and not being as interpretable.</p><p>In the next section, we will walk through an example on how to create your simulated user feedback.</p><h3>How It Works</h3><p>In this section we will show how we can use the open-source library <a href="https://github.com/relari-ai/continuous-eval">continuous-eval</a> to create simulated user feedback.</p><p>Consider a Q&amp;A chatbot application. After deployment, users begin rating responses with thumbs up or down, indicating a need for performance enhancement. For this example we will use the example named correctness in continuous-eval:</p><pre>dataset = Dataset(example_data_downloader(&quot;correctness&quot;))<br><br># Samples are annotated with &quot;correct&quot;, &quot;incorrect&quot; or &quot;refuse-to-answer&quot;<br># We remove the samples where the LLL refused to answer (i.e., said &quot;I don&#39;t know&quot;)<br>dataset.filter(lambda x: x[&quot;annotation&quot;] != &quot;refuse-to-answer&quot;)<br>dataset.sample(300) # Only for this example: randomly sample 300 examples</pre><p>As we mentioned, we want to create some custom criteria. We leverage the LLMBasedCustomMetric class to define the <em>Tone</em> and <em>Conciseness</em> metrics. To do so we need to define the metric and provide a scoring rubric.</p><p>For the tone:</p><pre>tone = LLMBasedCustomMetric(<br> name=&quot;Tone&quot;,<br> definition=&quot;The Tone/Content Issues metric evaluates the appropriateness and accuracy of the tone and content in responses to specific questions. It focuses on ensuring that the tone is professional and suitable for the context, and that the content accurately addresses the question without unnecessary deviations or inaccuracies. This metric is crucial for maintaining a professional image and ensuring clear, direct communication.&quot;,<br> scoring_rubric=&quot;&quot;&quot;Use the following rubric to assign a score to the answer based on its tone:<br>- Score 1: The response is inappropriate or inaccurate, with a tone that is either too informal, overly strong, or not suited to the professional context. The content may be irrelevant, incorrect, or fail to directly address the question posed.<br>- Score 2: The response is mostly appropriate and accurate but may contain minor tone or content issues. The tone is generally professional but may slip into informality or unnecessary strength in places. The content addresses the question but may include minor inaccuracies or unnecessary details.<br>- Score 3: The response is appropriate and accurate, with a tone that is professional and suited to the context. The content directly and correctly addresses the question without unnecessary deviations or inaccuracies.&quot;&quot;&quot;,<br> scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),<br> model_parameters={&quot;temperature&quot;: 0},<br>)</pre><p>while for conciseness:</p><pre>conciseness = LLMBasedCustomMetric(<br> name=&quot;Conciseness&quot;,<br> definition=&quot;Conciseness in communication refers to the expression of ideas in a clear and straightforward manner, using the fewest possible words without sacrificing clarity or completeness of information. It involves eliminating redundancy, verbosity, and unnecessary details, focusing instead on delivering the essential message efficiently. &quot;,<br> scoring_rubric=&quot;&quot;&quot;Use the following rubric to assign a score to the answer based on its conciseness:<br>- Score 1: The answer is overly verbose, containing a significant amount of unnecessary information, repetition, or redundant expressions that do not contribute to the understanding of the topic.<br>- Score 2: The answer includes some unnecessary details or slightly repetitive information, but the excess does not severely hinder understanding.<br>- Score 3: The answer is clear, direct, and to the point, with no unnecessary words, details, or repetition.&quot;&quot;&quot;,<br> scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),<br> model_parameters={&quot;temperature&quot;: 0},<br>)</pre><p>We use <em>Tone</em> and <em>Conciseness</em> together with more standard metrics, in particular we will consider the</p><ul><li>Answer Correctness (DeterministicAnswerCorrectens and LLMBasedAnswerCorrectness)</li><li>Answer Relevance (LLMBasedAnswerRelevance)</li><li>Style Consistency (LLMBasedStyleConsistency)</li><li>Readability (FleschKincaidReadability)</li></ul><p>The next step is to put all the metrics together and specify what field of the dataset should be used to compute the metrics. To do that we can use the SingleModulePipeline</p><pre>pipeline = SingleModulePipeline(<br> dataset=dataset,<br> eval=[<br> DeterministicAnswerCorrectness().use(<br> answer=dataset.answer,<br> ground_truth_answers=dataset.ground_truths,<br> ),<br> LLMBasedAnswerCorrectness().use(<br> question=dataset.question,<br> answer=dataset.answer,<br> ground_truth_answers=dataset.ground_truths,<br> ),<br> LLMBasedAnswerRelevance().use(<br> question=dataset.question, answer=dataset.answer<br> ),<br> LLMBasedStyleConsistency().use(<br> answer=dataset.answer, ground_truth_answers=dataset.ground_truths<br> ),<br> FleschKincaidReadability().use(answer=dataset.answer),<br> tone.use(<br> question=dataset.question,<br> answer=dataset.answer,<br> ground_truth_answers=dataset.ground_truths,<br> ),<br> conciseness.use(<br> question=dataset.question,<br> answer=dataset.answer,<br> ground_truth_answers=dataset.ground_truths,<br> ),<br> ],<br>)</pre><p>and run all the metrics using the EvaluationManager</p><pre>eval_manager = EvaluationManager(pipeline)<br># The dataset already contains the model output so we just set the evaluation results<br>eval_manager.evaluation.results = dataset.data<br>eval_manager.run_metrics() # Note: there is no progress bar, it might take a few minutes</pre><p>The next step is to train simulated user feedback predictor</p><pre>datasplit = DataSplit(<br> X=eval_manager.metrics.to_pandas(),<br> y=map(lambda x: 1 if x == &quot;correct&quot; else 0, dataset[&quot;annotation&quot;]),<br> split_ratios=SplitRatios(train=0.6, test=0.2, calibration=0.2),<br>)<br><br># We use the train and calibration sets to train the classifier<br>predictor = EnsembleMetric(training=datasplit.train, calibration=datasplit.calibration)</pre><p>This simulated user feedback predictor is able to correctly predict the human feedback in the test split 96.67% of the time.</p><p>We can leverage the proposed approach to better understand what is important to the user. Below is the learned importance of every metric by the simulated user feedback predictor.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*DisAnfk1AAm5pGI2VJ1vzQ.png" /><figcaption>Learned importance of every metric by the simulated user feedback predictor. Image by the author.</figcaption></figure><p>Looking at the plot, we see that <em>Correctness</em> (including <em>token overlap</em>, which is another measure for correctness) and <em>Relevance</em> to the question are the most important predictors of user preference. But the user also weighs <em>tone</em> and <em>style consistency</em> into the decision. At the same time, we can see that <em>conciseness</em> and <em>readability</em> are not as important. Reviewing this graph provides valuable insight into user preferences, giving a clear indication of what elements are essential and what can be adjusted if compromises need to be made.</p><h3>Wrapping Up</h3><p>Collecting user feedback is challenging, yet it is the most important information for developers of large language models (LLMs). By simulating user feedback during offline testing, we significantly reduces the time it takes for feedback to travel from the field back to developers, while maintaining positive user relationships.</p><p>In practice, our approach has proven to closely mirror actual human responses, outperforming traditional methods that rely on isolated LLM responses. This strategy allows for the incremental improvement of generative AI applications, fostering continuous refinement and greater congruence with what users expect.</p><p>—</p><p>Note: We will soon publish a research paper with more details on this methodology. Stay tuned!</p><h3>Coming Next</h3><ul><li>Techniques for curating golden dataset</li><li>How to make the most out of your Embedding Model?</li><li>What data should I use for Fine-Tuning?</li></ul><h3>Earlier Posts</h3><ul><li><a href="https://medium.com/p/27a472b09893">Practical Guide to RAG Pipeline Evaluation Part 1: Retrieval</a></li><li><a href="https://medium.com/relari/a-practical-guide-to-rag-evaluation-part-2-generation-c79b1bde0f5d">Practical Guide to RAG Pipeline Evaluation Part 2: Generation</a></li><li><a href="https://medium.com/relari/how-important-is-a-golden-dataset-for-llm-pipeline-evaluation-4ef6deb14dc5">How important is a Golden Dataset for LLM pipeline evaluation?</a></li><li><a href="https://medium.com/relari/case-study-reference-free-vs-reference-based-evaluation-of-rag-pipeline-9a49ef49866c">Case Study: Reference-free vs Reference-based evaluation</a></li><li><a href="https://medium.com/relari/how-to-evaluate-complex-genai-apps-a-granular-approach-0ab929d5b3e2">How to evaluate complex GenAI Apps: a granular approach</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=843c444febc7" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/how-to-make-the-most-out-of-llm-production-data-simulated-user-feedback-843c444febc7">How to make the most out of LLM production data: simulated user feedback</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
            </content:encoded>
        </item>
        <item>
            <title>
                <![CDATA[ Why Human-Centred Approaches Lead to Better Algorithm Design ]]>
            </title>
            <description>
                <![CDATA[ <div class="medium-feed-item"><p class="medium-feed-image"><a href="https://towardsdatascience.com/why-human-centred-approaches-lead-to-better-algorithm-design-44f9ff8cf352?source=rss----7f60cf5620c9---4"><img src="https://cdn-images-1.medium.com/max/2038/1*Bnn5VOEPtuErMEt8IhUmWg.png" width="2038"></a></p><p class="medium-feed-snippet">Some lessons I learned about quantifying qualitative thinking</p><p class="medium-feed-link"><a href="https://towardsdatascience.com/why-human-centred-approaches-lead-to-better-algorithm-design-44f9ff8cf352?source=rss----7f60cf5620c9---4">Continue reading on Towards Data Science »</a></p></div> ]]>
            </description>
            <link>https://towardsdatascience.com/why-human-centred-approaches-lead-to-better-algorithm-design-44f9ff8cf352?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/44f9ff8cf352</guid>
            <category>
                <![CDATA[ data-analysis ]]>
            </category>
            <category>
                <![CDATA[ algorithms ]]>
            </category>
            <category>
                <![CDATA[ human-centered-design ]]>
            </category>
            <category>
                <![CDATA[ data-science ]]>
            </category>
            <category>
                <![CDATA[ qualitative-research ]]>
            </category>
            <dc:creator>
                <![CDATA[ John Loewen, PhD ]]>
            </dc:creator>
            <pubDate>Thu, 11 Apr 2024 03:30:08 GMT</pubDate>
            <atom:updated>2024-04-11T03:30:07.857Z</atom:updated>
        </item>
        <item>
            <title>
                <![CDATA[ Scaling AI Models Like You Mean It ]]>
            </title>
            <link>https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/3afa56c1e14b</guid>
            <category>
                <![CDATA[ ai ]]>
            </category>
            <category>
                <![CDATA[ cloud ]]>
            </category>
            <category>
                <![CDATA[ deployment ]]>
            </category>
            <category>
                <![CDATA[ mlops ]]>
            </category>
            <category>
                <![CDATA[ open-source ]]>
            </category>
            <dc:creator>
                <![CDATA[ Sean Sheng ]]>
            </dc:creator>
            <pubDate>Wed, 10 Apr 2024 22:20:04 GMT</pubDate>
            <atom:updated>2024-04-10T22:20:04.877Z</atom:updated>
            <content:encoded>
                <![CDATA[ <h4>Strategies for Overcoming the Challenges of Scaling Open-Source AI Models in Production</h4><p>If you’re reading this article, you probably need no introduction to the advantages of deploying open-source models. Over the past couple of years, we have seen incredible growth in the both the quantity and quality of open source models.</p><ul><li>Platforms such as Hugging Face have democratized access to a wide array of models, including Large Language Models (LLMs) and diffusion models, empowering developers to innovate freely and efficiently.</li><li>Developers enjoy greater autonomy, as they can fine-tune and combine different models at will, leading to innovative approaches like Retrieval-Augmented Generation (RAG) and the creation of advanced agents.</li><li>From an economic perspective, open-source models provide substantial cost savings, enabling the use of smaller, specialized models that are more budget-friendly compared to general-purpose models like GPT-4.</li></ul><p>Open-source models present an attractive solution, but what’s the next hurdle? Unlike using a model endpoint like OpenAI, where the model is a scalable black box behind the API, deploying your own open-source models introduces scaling challenges. It’s crucial to ensure that your model scales effectively with production traffic and maintains a seamless experience during traffic spikes. Additionally, it’s important to manage costs efficiently, so you only pay for what you use and avoid any financial surprises at the end of the month.</p><h3>True north: Serverless functions for GPUs</h3><p>Interestingly, this sounds like a challenge that modern serverless architectures, like AWS Lambda, have already solved — a solution that have existed for almost a decade. However, when it comes to AI model deployment, this isn’t quite the case.</p><p>The limitations of serverless functions for AI deployments are multifaceted.</p><ul><li><strong>No GPU support</strong>. Platforms like AWS Lambda don’t support GPU. This isn’t merely a technical oversight; it’s rooted in architectural and practical considerations.</li><li><strong>GPUs cannot be easily shared.</strong> GPUs, while highly parallelizable as devices, is not as flexible in handling multiple inference tasks on different models simultaneously.</li><li><strong>GPUs are expensive</strong>. They’re exceptional for model inferencetasks but costly to maintain, especially if not utilized continuously.</li></ul><p>Next, let’s take a look at our scaling journey and the important lessons we have learned along the way.</p><h3>The cold start problem</h3><p>Before we could even begin to work on scaling, we have the notorious “cold start” problem. This issue presents itself in three different stages:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OL5eErDqMaNVHvqz.png" /><figcaption>Breakdown of the cold start problem. Image by the author.</figcaption></figure><ol><li><strong>Cloud provisioning</strong>: This phase involves the time it takes for a cloud provider to allocate an instance and integrate it into our cluster. This process varies widely, ranging from as quick as 30 seconds to several minutes, and in some cases, even hours, especially for high-demand instances like the Nvidia A100 and H100 GPUs.</li><li><strong>Container image pulling</strong>: Unlike simple Python job images, AI model serving images are very complex, due to the dependencies and custom libraries they require. Although cloud providers boast multi-gigabit network bandwidth, our experience often saw download speeds far below them, with image pulling time about 3 minutes.</li><li><strong>Model loading</strong>. The time required here is largely dependent on the model’s size, with larger models like LLMs and diffusion models taking significantly longer time due to their billions of parameters. For example, loading a 5GB model like Stable Diffusion 2 might take approximately 1.3 minutes with 1Gbps network bandwidth, while larger models like Llama 13B and Mixtral 8x7B could require 3.5 minutes and 12.5 minutes respectively.</li></ol><p>Each phase of the cold start issue demands specific strategies to minimize delays. In the following sections, we’ll explore each of them in more detail, sharing our strategies and solutions.</p><h4>Cloud provisioning</h4><p>In contrast to the homogeneous environment of serverless CPUs, managing a diverse range of compute instance types is crucial when dealing with GPUs, each tailored for specific use cases. For instance, IO-bound LLMs require high GPU memory bandwidth and capacity, while generative models need more powerful GPU compute.</p><p>Ensuring availability during peak traffic by maintaining all GPU instance types could lead to prohibitively high costs. To avoid the financial strain of idle instances, we implemented a “standby instances” mechanism. Rather than preparing for the maximum potential load, we maintained a calculated number of standby instances that match the incremental scaling step sizes. For example, if we scale by two GPUs at a time, we need to have two standby instances ready. This allows us to quickly add resources to our serving fleet as demand surges, significantly reducing wait time, while keeping cost manageable.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5CfN5iEju2-rn6tF.png" /><figcaption>Image by the author.</figcaption></figure><p>In a multi-tenant environment, where multiple teams or, in our case, multiple organizations, share a common resource pool, we can achieve more efficient utilization rates. This shared environment allows us to balance varying resource demands, contributing to improved cost efficiency. However, managing multi-tenancy introduces challenges, such as enforcing quotas and ensuring network isolation, which can add complexity to the cluster.</p><h4>Container image pulling</h4><p>Serverless CPU workloads often use lightweight images, like the Python slim image (around 154 MB). In stark contrast, a container image built for serving an LLM can be much larger (6.7 GB); the bulk of this size comes from the various dependencies required to run the AI model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*799lkwOqT-BYZiKP.png" /><figcaption>Image by the author.</figcaption></figure><p>Despite high-bandwidth networks advertised by cloud providers, the reality often falls short, with actual download speeds being a fraction of the promised rates.</p><p>Practically, a significant portion of the files were never used. One way is to optimize the container image itself, but that quickly proved to be unmanageable. Instead, we shifted our focus to an on-demand file pulling approach. Specifically, we first downloaded only the image metadata, with the actual remote files being fetched later as needed. In addition, we leveraged peer-to-peer networking within the cluster to dramatically increase pulling efficiency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*We_KIJXR5gdsBOe-.png" /><figcaption>Container image metadata can be pull in seconds. Image by the author.</figcaption></figure><p>With these optimizations, we reduced the image pulling time from several minutes to mere seconds. However, we all know this measurement is “cheating” since the actual files are not pulled at this stage. The real file pulling occurs when the service runs. Therefore, it’s crucial to have a service framework that allows you to define behaviors at various lifecycle stages, such as initialization and serving. By doing all of the bootstrapping during initialization, we can ensure that all file dependencies are pulled. This way, when it comes to serving time, there are no delays caused by file pulling.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/824/0*x6hJUxbPegtJqt_H.png" /><figcaption>Service framework that enables service initialization and API definitions. Image by the author.</figcaption></figure><p>In the above example, model loading is done during the initialization lifecycle within __init__ and serving happens within the @bentoml.api named txt2img.</p><h4>Model loading</h4><p>Initially, the most straightforward method for model loading was to fetch it directly from a remote store like Hugging Face. Using Content Delivery Networks (CDNs), NVMe SSDs, and shared memory, we could remove some of the bottlenecks. While this worked, it was far from optimal.</p><p>To improve this process, we considered using in-region network bandwidth. We seeded models in our distributed file systems and broke them into smaller chunks, allowing for parallel downloads. This drastically improved performance, but we still encountered cloud provider’s network bandwidth bottlenecks.</p><p>In response, we further optimized to leverage in-cluster network bandwidth by using peer-to-peer sharing and tapping into local caches. While the improvements were substantial, they added a layer of complexity to the process, which we need to abstract away from the developers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/673/0*OnXqZdd22lnaV5tR.png" /><figcaption>Image by the author.</figcaption></figure><p>Even with the above practices, we still suffer from a sequential bottleneck: the need to wait for each step to complete before proceeding with the next. Models had to be downloaded to persistent drive entirely before loading into CPU memory, and then into the GPU.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gjzKBXni-LJPMt2x.png" /><figcaption>Image by the author.</figcaption></figure><p>We turned to a stream-based method for loading model weights, using the distributed file cache system we had in place. This system allows programs to operate as if all files were logically available on disk. In reality, the required data is fetched on-demand from remote storage therefore bypassed disk writing. By leveraging a format like <a href="https://github.com/huggingface/safetensors">Safetensors</a>, we can efficiently load the model weights into the main memory through memory mapping (mmap) before loading to the GPU memory in a streaming fashion.</p><p>Moreover, we adopted asynchronous writing to disk. By doing so, we created a faster-access cache layer on the local disk. Thus, new deployments with only code changes could bypass the slower remote storage fetch phase, reading the model weights from local cache directly.</p><p>To summarize, we managed to optimize the cold start time and we were happy with the results:</p><ul><li><strong>No cloud provision delay</strong> with standby instances.</li><li><strong>Faster container image pulling</strong> with on-demand and peer-to-peer streaming.</li><li><strong>Accelerated model loading</strong> time with distributed file systems, peer-to-peer caching, and streamed loading to GPU memory.</li><li><strong>Parallelized</strong> image pulling and model loading enabled by service framework.</li></ul><h3>Scaling metrics</h3><p>Next, we need to identify the most indicative signal for scaling AI model deployments on GPUs.</p><h4>Resource utilization metrics</h4><p>Initially, we considered CPU utilization. It’s straightforward and has an intuitive default threshold, such as 80%. However, the obvious drawback is that CPU metrics don’t capture GPU utilization. Additionally, the Global Interpreter Lock (GIL) in Python limits parallelism, preventing high CPU utilization on multi-core instances, making CPU utilization a less feasible metric.</p><p>We also explored GPU utilization as a more direct measure of our models’ workloads. However, we encountered an issue: the GPU utilization reported by tools like nvml didn&#39;t accurately represent the actual utilization of the GPU. This metric samples kernel usage over a period of time, and a GPU is considered utilized if at least one kernel is executing. This aligns with our observation that better performance can often be achieved through improved batching, even though the GPU device was already reported as having high utilization.</p><blockquote><strong><em>Note</em></strong><em>: According to </em><a href="https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries"><em>the NVIDIA documentation</em></a><em>, utilization.gpu means “Percent of time over the past sample period during which one or more kernels was executing on the GPU. The sample period may be between 1 second and 1/6 second depending on the product”.</em></blockquote><p>Resource-based metrics are inherently retrospective as they only reflect usage after the resources have been consumed. They’re also capped at 100%, which presents a problem: when scaling based on these metrics, the maximum ratio for adjustment is typically the current utilization over the desired threshold (see scaling formula below). This results in a conservative scale-up behavior that doesn’t necessarily match the actual demand of production traffic.</p><pre>desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]</pre><h4>Request-based metrics</h4><p>We turned to request-based metrics for more proactive signaling that are also not capped at a 100%.</p><p>QPS is a widely recognized metric for its simplicity. However, its application in generative AI, such as with LLMs, is still a question. QPS is not easy to configure and due to the variable cost per request, which depends on the number of tokens processed and generated, using QPS as a scaling metric can lead to inaccuracies.</p><p>Concurrency, on the other hand, has proven to be an ideal metric for reflecting the actual load on the system. It represents the number of active requests either queued or being processed. This metric:</p><ul><li>Precisely reflects the load on the system. <a href="https://en.wikipedia.org/wiki/Little%27s_law">Little’s Law</a>, which states that QPS multiplied by average latency equals concurrency, provides an elegant way to understand the relationship between QPS and concurrency. In practice, the average latency per request is rather unknown in model serving. However, by measuring concurrency, we don’t need to calculate average latency.</li><li>Accurately calculate the desired replicas using the scaling formula. Allowing the deployment to directly scale to the ideal size without intermediate steps.</li><li>Easy to configure based on batch size. For non-batchable models, it’s simply the number of GPUs, since each can only handle one generation task at a time. For models that support batching, the batch size determines the concurrency level.</li></ul><p>For concurrency to work, we need the support from the service framework to automatically instrument concurrency as a metric and serve it as a scaling signal for the deployment platform. We must also establish right scaling policies to help against overzealous scale-up during a traffic spike or premature scale-down when traffic is sparse.</p><h3>Request queue</h3><p>A another important mechanism we integrated with concurrency is the request queue. It acts as a buffer and an orchestrator, ensuring that incoming requests are handled efficiently and without overloading any single server replica.</p><p>In a scenario without a request queue, all incoming requests are dispatched directly to the server (6 requests in the image below). If multiple requests arrive simultaneously, and there’s only one active server replica, it becomes a bottleneck. The server tries to process each request in a first-come-first-serve manner, often leading to timeouts and a bad client experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/975/0*fttyHjlZWOzp0fTv.png" /><figcaption>Image by the author.</figcaption></figure><p>Conversely, with a request queue in place, the server consumes requests at an optimal rate, processing at a rate based on the <em>concurrency</em> defined for the service. When additional server replicas scale up, they too begin to pull from the queue. This mechanism prevents any single server from becoming overwhelmed and allows for a smoother, more manageable distribution of requests across the available infrastructure.</p><h3>Conclusions</h3><p>Our journey in exploring AI model scaling solutions has been an adventure, which has led us to ultimately create the scaling experience on BentoCloud — a platform that encapsulates all our learnings.</p><p>To avoid the impression of a promotion, we’ll illustrate our point with a picture that’s worth a thousand words. The monitoring dashboard below demonstrates the correlation between incoming requests and the scaling up of server instances.</p><p>Equally important to scaling up is the ability to scale down. As the requests waned to zero, the deployment reduced the number of active instances accordingly. This ability ensures that no unnecessary costs are incurred for unused resources, aligning expenditure with actual usage.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*E5cOlyCuLe2mY4tW.png" /><figcaption>BentoCloud monitoring dashboard. Image by the author.</figcaption></figure><p>We hope the takeaway is that scaling for model deployments should be considered an important aspect of production applications. Unlike scaling CPU workloads, scaling model deployments on GPUs presents unique challenges, including cold start times, configuring scaling metrics, and orchestrating requests. When evaluating deployment platforms, their solutions to these challenges should be thoroughly assessed.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3afa56c1e14b" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b">Scaling AI Models Like You Mean It</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
            </content:encoded>
        </item>
        <item>
            <title>
                <![CDATA[ Feature Engineering with Microsoft Fabric and PySpark ]]>
            </title>
            <link>https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/16d458018744</guid>
            <category>
                <![CDATA[ microsoft-fabric ]]>
            </category>
            <category>
                <![CDATA[ data-science ]]>
            </category>
            <category>
                <![CDATA[ feature-engineering ]]>
            </category>
            <category>
                <![CDATA[ data-engineering ]]>
            </category>
            <category>
                <![CDATA[ basketball ]]>
            </category>
            <dc:creator>
                <![CDATA[ Roger Noble ]]>
            </dc:creator>
            <pubDate>Wed, 10 Apr 2024 17:27:24 GMT</pubDate>
            <atom:updated>2024-04-10T17:27:24.548Z</atom:updated>
            <content:encoded>
                <![CDATA[ <h4>Fabric Madness part 2</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xBVdP3mItqXf4DOFdlnkJQ.png" /><figcaption>Image by author and ChatGPT. “Design an illustration, focusing on a basketball player in action, this time the theme is on using pyspark to generate features for machine leaning models in a graphic novel style” prompt. ChatGPT, 4, OpenAI, 4 April. 2024. <a href="https://chat.openai.com./">https://chat.openai.com.</a></figcaption></figure><p><em>A Huge thanks to </em><a href="https://medium.com/@mgrc99"><em>Martim Chaves</em></a><em> who co-authored this post and developed the example scripts.</em></p><p>In our <a href="https://medium.com/towards-data-science/fabric-madness-96b84dc5f241">previous post</a> we took a high level view of how to train a machine learning model in <a href="https://www.microsoft.com/en-us/microsoft-fabric">Microsoft Fabric</a>. In this post we wanted to dive deeper into the process of feature engineering.</p><p>Feature engineering is a crucial part of the development lifecycle for any Machine Learning (ML) systems. It is a step in the development cycle where raw data is processed to better represent its underlying structure and provide additional information that enhance our ML models. Feature engineering is both an art and a science. Even though there are specific steps that we can take to create good features, sometimes, it is only through experimentation that good results are achieved. Good features are crucial in guaranteeing a good system performance.</p><p>As datasets grow exponentially, traditional feature engineering may struggle with the size of very large datasets. This is where PySpark can help — as it is a scalable and efficient processing platform for massive datasets. A great thing about Fabric is that it makes using PySpark easy!</p><p>In this post, we’ll be going over:</p><ul><li>How does PySpark Work?</li><li>Basics of PySpark</li><li>Feature Engineering in Action</li></ul><p>By the end of this post, hopefully you’ll feel comfortable carrying out feature engineering with PySpark in Fabric. Let’s get started!</p><h3>How does PySpark work?</h3><p>Spark is a distributed computing system that allows for the processing of large datasets with speed and efficiency across a cluster of machines. It is built around the concept of a Resilient Distributed Dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. RDDs are the fundamental data structure of Spark, and they allow for the distribution of data across a cluster of machines.</p><p>PySpark is the Python API for Spark. It allows for the creation of Spark DataFrames, which are similar to Pandas DataFrames, but with the added benefit of being distributed across a cluster of machines. PySpark DataFrames are the core data structure in PySpark, and they allow for the manipulation of large datasets in a distributed manner.</p><p>At the core of PySpark is the SparkSession object, which is what fundamentally interacts with Spark. This SparkSession is what allows for the creation of DataFrames, and other functionalities. Note that, when running a Notebook in Fabric, a SparkSession is automatically created for you, so you don&#39;t have to worry about that.</p><p>Having a rough idea of how PySpark works, let’s get to the basics.</p><h3>Basics of PySpark</h3><p>Although Spark DataFrames may remind us of Pandas DataFrames due to their similarities, the syntax when using PySpark can be a bit different. In this section, we’ll go over some of the basics of PySpark, such as reading data, combining DataFrames, selecting columns, grouping data, joining DataFrames, and using functions.</p><h4>The Data</h4><p>The data we are looking at is from the 2024 US college basketball tournaments, which was obtained from the on-going <em>March Machine Learning Mania 2024</em> Kaggle competition, the details of which can be found <a href="https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview">here</a>, and is licensed under CC BY 4.0 [1]</p><h4>Reading data</h4><p>As mentioned in the <a href="https://medium.com/towards-data-science/fabric-madness-96b84dc5f241">previous post</a> of this series, the first step is usually to create a Lakehouse and upload some data. Then, when creating a Notebook, we can attach it to the created Lakehouse, and we’ll have access to the data stored there.</p><p>PySpark Dataframes can read various data formats, such as CSV, JSON, Parquet, and others. Our data is stored in CSV format, so we’ll be using that, like in the following code snippet:</p><pre># Read women&#39;s data<br>w_data = (<br> spark.read.option(&quot;header&quot;, True)<br> .option(&quot;inferSchema&quot;, True)<br> .csv(f&quot;Files/WNCAATourneyDetailedResults.csv&quot;)<br> .cache()<br>)</pre><p>In this code snippet, we’re reading the detailed results data set of the final women’s basketball college tournament matches. Note that the &quot;header&quot; option being true means that the names of the columns will be derived from the first row of the CSV file. The inferSchema option tells Spark to guess the data types of the columns - otherwise they would all be read as strings. .cache() is used to keep the DataFrame in memory.</p><p>If you’re coming from Pandas, you may be wondering what the equivalent of df.head() is for PySpark - it&#39;s df.show(5). The default for .show() is the top 20 rows, hence the need to specifically select 5.</p><h4>Combining DataFrames</h4><p>Combining DataFrames can be done in multiple ways. The first we will look at is a union, where the columns are the same for both DataFrames:</p><pre># Read women&#39;s data<br>...<br><br># Read men&#39;s data<br>m_data = (<br> spark.read.option(&quot;header&quot;, True)<br> .option(&quot;inferSchema&quot;, True)<br> .csv(f&quot;Files/MNCAATourneyDetailedResults.csv&quot;)<br> .cache()<br>)<br><br># Combine (union) the DataFrames<br>combined_results = m_data.unionByName(w_data)</pre><p>Here, unionByName joins the two DataFrames by matching the names of the columns. Since both the women&#39;s and the men&#39;s <em>detailed match results</em> have the same columns, this is a good approach. Alternatively, there&#39;s also union, which combines two DataFrames, matching column positions.</p><h4>Selecting Columns</h4><p>Selecting columns from a DataFrame in PySpark can be done using the .select() method. We just have to indicate the name or names of the columns that are relevant as a parameter.</p><p>Here’s the output for w_scores.show(5):</p><pre># Selecting a single column<br>w_scores = w_data.select(&quot;WScore&quot;)<br><br># Selecting multiple columns<br>teamid_w_scores = w_data.select(&quot;WTeamID&quot;, &quot;WScore&quot;)<br>```<br><br>Here&#39;s the output for `w_scores.show(5)`:<br>```<br>+------+<br>|Season|<br>+------+<br>| 2010|<br>| 2010|<br>| 2010|<br>| 2010|<br>| 2010|<br>+------+<br>only showing top 5 rows</pre><p>The columns can also be renamed when being selected using the .alias() method:</p><pre>winners = w_data.select(<br> w_data.WTeamID.alias(&quot;TeamID&quot;),<br> w_data.WScore.alias(&quot;Score&quot;)<br>)</pre><h4>Grouping Data</h4><p>Grouping allows us to carry out certain operations for the groups that exist within the data and is usually combined with a aggregation functions. We can use .groupBy() for this:</p><pre># Grouping and aggregating<br>winners_average_scores = winners.groupBy(&quot;TeamID&quot;).avg(&quot;Score&quot;)</pre><p>In this example, we are grouping by &quot;TeamID&quot;, meaning we&#39;re considering the groups of rows that have a distinct value for &quot;TeamID&quot;. For each of those groups, we&#39;re calculating the average of the &quot;Score&quot;. This way, we get the average score for each team.</p><p>Here’s the output of winners_average_scores.show(5), showing the average score of each team:</p><pre>+------+-----------------+<br>|TeamID| avg(Score)|<br>+------+-----------------+<br>| 3125| 68.5|<br>| 3345| 74.2|<br>| 3346|79.66666666666667|<br>| 3376|73.58333333333333|<br>| 3107| 61.0|<br>+------+-----------------+</pre><h4>Joining Data</h4><p>Joining two DataFrames can be done using the .join() method. Joining is essentially extending the DataFrame by adding the columns of one DataFrame to another.</p><pre># Joining on Season and TeamID<br>final_df = matches_df.join(stats_df, on=[&#39;Season&#39;, &#39;TeamID&#39;], how=&#39;left&#39;)</pre><p>In this example, both stats_df and matches_df were using Season and TeamID as unique identifiers for each row. Besides Season and TeamID, stats_df has other columns, such as statistics for each team during each season, whereas matches_df has information about the matches, such as date and location. This operation allows us to add those interesting statistics to the matches information!</p><h4>Functions</h4><p>There are several functions that PySpark provides that help us transform DataFrames. You can find the full list <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html">here</a>.</p><p>Here’s an example of a simple function:</p><pre>from pyspark.sql import functions as F<br><br>w_data = w_data.withColumn(&quot;HighScore&quot;, F.when(F.col(&quot;Score&quot;) &gt; 80, &quot;Yes&quot;).otherwise(&quot;No&quot;))</pre><p>In the code snippet above, a &quot;HighScore&quot; column is created when the score is higher than 80. For each row in the &quot;Score&quot; column (indicated by the .col() function), the value &quot;Yes&quot; is chosen for the &quot;HighScore&quot; column if the &quot;Score&quot; value is larger than 80, determined by the .when() function. .otherwise(), the value chosen is &quot;No&quot;.</p><h3>Feature Engineering in Action</h3><p>Now that we have a basic understanding of PySpark and how it can be used, let’s go over how the regular season statistics features were created. These features were then used as inputs into our machine learning model to try to predict the outcome of the final tournament games.</p><p>The starting point was a DataFrame, regular_data, that contained match by match statistics for the <em>regular seasons</em>, which is the United States College Basketball Season that happens from November to March each year.</p><p>Each row in this DataFrame contained the season, the day the match was held, the ID of team 1, the ID of team 2, and other information such as the location of the match. Importantly, it also contained statistics for <em>each team</em> for that <em>specific match</em>, such as &quot;T1_FGM&quot;, meaning the Field Goals Made (FGM) for team 1, or &quot;T2_OR&quot;, meaning the Offensive Rebounds (OR) of team 2.</p><p>The first step was selecting which columns would be used. These were columns that strictly contained in-game statistics.</p><pre># Columns that we&#39;ll want to get statistics from<br>boxscore_cols = [<br> &#39;T1_FGM&#39;, &#39;T1_FGA&#39;, &#39;T1_FGM3&#39;, &#39;T1_FGA3&#39;, &#39;T1_OR&#39;, &#39;T1_DR&#39;, &#39;T1_Ast&#39;, &#39;T1_Stl&#39;, &#39;T1_PF&#39;, <br> &#39;T2_FGM&#39;, &#39;T2_FGA&#39;, &#39;T2_FGM3&#39;, &#39;T2_FGA3&#39;, &#39;T2_OR&#39;, &#39;T2_DR&#39;, &#39;T2_Ast&#39;, &#39;T2_Stl&#39;, &#39;T2_PF&#39;<br>]</pre><p>If you’re interested, here’s what each statistic’s code means:</p><ul><li>FGM: Field Goals Made</li><li>FGA: Field Goals Attempted</li><li>FGM3: Field Goals Made from the 3-point-line</li><li>FGA3: Field Goals Attempted for 3-point-line goals</li><li>OR: Offensive Rebounds. A rebounds is when the ball rebounds from the board when a goal is attempted, not getting in the net. If the team that <em>attempted</em> the goal gets possession of the ball, it’s called an “Offensive” rebound. Otherwise, it’s called a “Defensive” Rebound.</li><li>DR: Defensive Rebounds</li><li>Ast: Assist, a pass that led directly to a goal</li><li>Stl: Steal, when the possession of the ball is stolen</li><li>PF: Personal Foul, when a player makes a foul</li></ul><p>From there, a dictionary of <em>aggregation expressions</em> was created. Basically, for each column name in the previous list of columns, a function was stored that would calculate the mean of the column, and rename it, by adding a suffix, &quot;mean&quot;.</p><pre>from pyspark.sql import functions as F<br>from pyspark.sql.functions import col # select a column<br><br>agg_exprs = {col: F.mean(col).alias(col + &#39;mean&#39;) for col in boxscore_cols}</pre><p>Then, the data was grouped by &quot;Season&quot; and &quot;T1_TeamID&quot;, and the aggregation functions of the previously created dictionary were used as the argument for .agg().</p><pre>season_statistics = regular_data.groupBy([&quot;Season&quot;, &quot;T1_TeamID&quot;]).agg(*agg_exprs.values())</pre><p>Note that the grouping was done by season and the <strong>ID of team 1</strong> — this means that &quot;T2_FGAmean&quot;, for example, will actually be the mean of the Field Goals Attempted made by the <strong>opponents</strong> of T1, not necessarily of a specific team. So, we actually need to rename the columns that are something like &quot;T2_FGAmean&quot; to something like &quot;T1_opponent_FGAmean&quot;.</p><pre># Rename columns for T1<br>for col in boxscore_cols:<br> season_statistics = season_statistics.withColumnRenamed(col + &#39;mean&#39;, &#39;T1_&#39; + col[3:] + &#39;mean&#39;) if &#39;T1_&#39; in col \<br> else season_statistics.withColumnRenamed(col + &#39;mean&#39;, &#39;T1_opponent_&#39; + col[3:] + &#39;mean&#39;)</pre><p>At this point, it’s important to mention that the regular_data DataFrame actually has <strong>two</strong> rows per each match that occurred. This is so that both teams can be &quot;T1&quot; and &quot;T2&quot;, for each match. This little &quot;trick&quot; is what makes these statistics useful.</p><p>Note that we “only” have the statistics for “T1”. We “need” the statistics for “T2” as well — “need” in quotations because there are no new statistics being calculated. We just need the same data, but with the columns having different names, so that for a match with “T1” and “T2”, we have statistics for both T1 and T2. So, we created a mirror DataFrame, where, instead of “T1&amp;mldr;mean” and “T1_opponent_&amp;mldr;mean”, we have “T2&amp;mldr;mean” and “T2_opponent_&amp;mldr;mean”. This is important because, later on, when we’re joining these regular season statistics to tournament matches, we’ll be able to have statistics for both team 1 <strong>and</strong> team 2.</p><pre>season_statistics_T2 = season_statistics.select(<br> *[F.col(col).alias(col.replace(&#39;T1_opponent_&#39;, &#39;T2_opponent_&#39;).replace(&#39;T1_&#39;, &#39;T2_&#39;)) if col not in [&#39;Season&#39;] else F.col(col) for col in season_statistics.columns]<br>)</pre><p>Now, there are two DataFrames, with season statistics for “both” T1 and T2. Since the final DataFrame will contain the “Season”, the “T1TeamID” and the “T2TeamID”, we can join these newly created features with a join!</p><pre>tourney_df = tourney_df.join(season_statistics, on=[&#39;Season&#39;, &#39;T1_TeamID&#39;], how=&#39;left&#39;)<br>tourney_df = tourney_df.join(season_statistics_T2, on=[&#39;Season&#39;, &#39;T2_TeamID&#39;], how=&#39;left&#39;)</pre><h4>Elo Ratings</h4><p>First created by <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Arpad Elo</a>, Elo is a rating system for zero-sum games (games where one player wins and the other loses), like basketball. With the Elo rating system, each team has an Elo rating, a value that generally conveys the team’s quality. At first, every team has the same Elo, and whenever they win, their Elo increases, and when they lose, their Elo decreases. A key characteristic of this system is that this value increases more with a win against a strong opponent than with a win against a weak opponent. Thus, it can be a very useful feature to have!</p><p>We wanted to capture the Elo rating of a team at the end of the regular season, and use that as feature for the tournament. To do this, we calculated the Elo for each team on a per match basis. To calculate Elo for this feature, we found it more straightforward to use Pandas.</p><p>Central to Elo is calculating the expected score for each team. It can be described in code like so:</p><pre># Function to calculate expected score<br>def expected_score(ra, rb):<br> # ra = rating (Elo) team A<br> # rb = rating (Elo) team B<br> # Elo function<br> return 1 / (1 + 10 ** ((rb - ra) / 400))</pre><p>Considering a team A and a team B, this function computes the expected score of team A against team B.</p><p>For each match, we would update the teams’ Elos. Note that the location of the match also played a part — winning at home was considered less impressive than winning away.</p><pre># Function to update Elo ratings, keeping T1 and T2 terminology<br>def update_elo(t1_elo, t2_elo, location, T1_Score, T2_Score):<br> expected_t1 = expected_score(t1_elo, t2_elo)<br> expected_t2 = expected_score(t2_elo, t1_elo)<br> <br> actual_t1 = 1 if T1_Score &gt; T2_Score else 0<br> actual_t2 = 1 - actual_t1<br><br> # Determine K based on game location<br> # The larger the K, the bigger the impact<br> # team1 winning at home (location=1) less impressive than winning away (location = -1)<br> if actual_t1 == 1: # team1 won<br> if location == 1:<br> k = 20<br> elif location == 0:<br> k = 30<br> else: # location = -1<br> k = 40<br> else: # team2 won<br> if location == 1:<br> k = 40<br> elif location == 0:<br> k = 30<br> else: # location = -1<br> k = 20<br> <br> new_t1_elo = t1_elo + k * (actual_t1 - expected_t1)<br> new_t2_elo = t2_elo + k * (actual_t2 - expected_t2)<br> <br> return new_t1_elo, new_t2_elo</pre><p>To apply the Elo rating system, we iterated through each season’s matches, initializing teams with a base rating and updating their ratings match by match. The final Elo available for each team in each season will, hopefully, be a good descriptor of the team’s quality.</p><pre>def calculate_elo_through_seasons(regular_data):<br><br> # For this feature, using Pandas<br> regular_data = regular_data.toPandas()<br> <br> # Set value of initial elo<br> initial_elo = 1500<br><br> # DataFrame to collect final Elo ratings<br> final_elo_list = []<br><br> for season in sorted(regular_data[&#39;Season&#39;].unique()):<br> print(f&quot;Season: {season}&quot;)<br> # Initialize elo ratings dictionary<br> elo_ratings = {}<br><br> print(f&quot;Processing Season: {season}&quot;)<br> # Get the teams that played in the season<br> season_teams = set(regular_data[regular_data[&#39;Season&#39;] == season][&#39;T1_TeamID&#39;]).union(set(regular_data[regular_data[&#39;Season&#39;] == season][&#39;T2_TeamID&#39;]))<br> <br> # Initialize season teams&#39; Elo ratings<br> for team in season_teams:<br> if (season, team) not in elo_ratings:<br> elo_ratings[(season, team)] = initial_elo<br><br> # Update Elo ratings per game<br> season_games = regular_data[regular_data[&#39;Season&#39;] == season]<br> for _, row in season_games.iterrows():<br> t1_elo = elo_ratings[(season, row[&#39;T1_TeamID&#39;])]<br> t2_elo = elo_ratings[(season, row[&#39;T2_TeamID&#39;])]<br><br> new_t1_elo, new_t2_elo = update_elo(t1_elo, t2_elo, row[&#39;location&#39;], row[&#39;T1_Score&#39;], row[&#39;T2_Score&#39;])<br> <br> # Only keep the last season rating<br> elo_ratings[(season, row[&#39;T1_TeamID&#39;])] = new_t1_elo<br> elo_ratings[(season, row[&#39;T2_TeamID&#39;])] = new_t2_elo<br><br> # Collect final Elo ratings for the season<br> for team in season_teams:<br> final_elo_list.append({&#39;Season&#39;: season, &#39;TeamID&#39;: team, &#39;Elo&#39;: elo_ratings[(season, team)]})<br><br> # Convert list to DataFrame<br> final_elo_df = pd.DataFrame(final_elo_list)<br><br> # Separate DataFrames for T1 and T2<br> final_elo_t1_df = final_elo_df.copy().rename(columns={&#39;TeamID&#39;: &#39;T1_TeamID&#39;, &#39;Elo&#39;: &#39;T1_Elo&#39;})<br> final_elo_t2_df = final_elo_df.copy().rename(columns={&#39;TeamID&#39;: &#39;T2_TeamID&#39;, &#39;Elo&#39;: &#39;T2_Elo&#39;})<br><br> # Convert the pandas DataFrames back to Spark DataFrames<br> final_elo_t1_df = spark.createDataFrame(final_elo_t1_df)<br> final_elo_t2_df = spark.createDataFrame(final_elo_t2_df)<br><br> return final_elo_t1_df, final_elo_t2_df</pre><p>Ideally, we wouldn’t calculate Elo changes on a match-by-match basis to determine each team’s final Elo for the season. However, we couldn’t come up with a better approach. Do you have any ideas? If so, let us know!</p><h4>Value Added</h4><p>The feature engineering steps demonstrated show how we can transform raw data — regular season statistics — into valuable information with predictive power. It is reasonable to assume that a team’s performance during the regular season is indicative of its potential performance in the final tournaments. By calculating the mean of observed match-by-match statistics for both the teams and their opponents, along with each team’s Elo rating in their final match, we were able to create a dataset suitable for modelling. Then, models were trained to predict the outcome of tournament matches using these features, among others developed in a similar way. With these models, we only need the two team IDs to look up the mean of their regular season statistics and their Elos to feed into the model and predict a score!</p><h3>Conclusion</h3><p>In this post, we looked at some of the theory behind Spark and PySpark, how that can be applied, and a concrete practical example. We explored how feature engineering can be done in the case of sports data, creating regular season statistics to use as features for final tournament games. Hopefully you’ve found this interesting and helpful — happy feature engineering!</p><p><strong>The full source code for this post and others in the series can be found </strong><a href="https://dev.azure.com/nobledynamic/_git/FabricMadness"><strong>here</strong></a><strong>.</strong></p><p><em>Originally published at </em><a href="https://nobledynamic.com/posts/fabric-madness-2/"><em>https://nobledynamic.com</em></a><em> on April 8, 2024.</em></p><h4>References</h4><p>[1] Jeff Sonas, Ryan Holbrook, Addison Howard, Anju Kandru. (2024). March Machine Learning Mania 2024. Kaggle. <a href="https://kaggle.com/competitions/march-machine-learning-mania-2024">https://kaggle.com/competitions/march-machine-learning-mania-2024</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=16d458018744" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744">Feature Engineering with Microsoft Fabric and PySpark</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
            </content:encoded>
        </item>
        <item>
            <title>
                <![CDATA[ ORPO: Preference Optimization without the Supervised Fine-tuning (SFT) Step ]]>
            </title>
            <description>
                <![CDATA[ <div class="medium-feed-item"><p class="medium-feed-image"><a href="https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=rss----7f60cf5620c9---4"><img src="https://cdn-images-1.medium.com/max/600/1*gEnWKniXhs3yygdOiM5sqA.png" width="600"></a></p><p class="medium-feed-snippet">A much cheaper alignment method performing as well as DPO</p><p class="medium-feed-link"><a href="https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=rss----7f60cf5620c9---4">Continue reading on Towards Data Science »</a></p></div> ]]>
            </description>
            <link>https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/60632ad0f450</guid>
            <category>
                <![CDATA[ data-science ]]>
            </category>
            <category>
                <![CDATA[ llm ]]>
            </category>
            <category>
                <![CDATA[ artificial-intelligence ]]>
            </category>
            <category>
                <![CDATA[ programming ]]>
            </category>
            <category>
                <![CDATA[ machine-learning ]]>
            </category>
            <dc:creator>
                <![CDATA[ Benjamin Marie ]]>
            </dc:creator>
            <pubDate>Wed, 10 Apr 2024 06:49:45 GMT</pubDate>
            <atom:updated>2024-04-10T06:49:45.283Z</atom:updated>
        </item>
        <item>
            <title>
                <![CDATA[ Overwriting in Python: Tricky. Dangerous. Powerful ]]>
            </title>
            <description>
                <![CDATA[ <div class="medium-feed-item"><p class="medium-feed-image"><a href="https://towardsdatascience.com/overwriting-in-python-tricky-dangerous-powerful-04b12a9b1a7e?source=rss----7f60cf5620c9---4"><img src="https://cdn-images-1.medium.com/max/2600/0*8XXJF7Lb_Avnrhi9" width="5116"></a></p><p class="medium-feed-snippet">Although overwriting objects is a typical Python coding technique, it can lead to unexpected effects. You need to know how to use it.</p><p class="medium-feed-link"><a href="https://towardsdatascience.com/overwriting-in-python-tricky-dangerous-powerful-04b12a9b1a7e?source=rss----7f60cf5620c9---4">Continue reading on Towards Data Science »</a></p></div> ]]>
            </description>
            <link>https://towardsdatascience.com/overwriting-in-python-tricky-dangerous-powerful-04b12a9b1a7e?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/04b12a9b1a7e</guid>
            <category>
                <![CDATA[ python ]]>
            </category>
            <category>
                <![CDATA[ python-programming ]]>
            </category>
            <category>
                <![CDATA[ programming ]]>
            </category>
            <category>
                <![CDATA[ deep-dives ]]>
            </category>
            <category>
                <![CDATA[ data-science ]]>
            </category>
            <dc:creator>
                <![CDATA[ Marcin Kozak ]]>
            </dc:creator>
            <pubDate>Wed, 10 Apr 2024 05:07:15 GMT</pubDate>
            <atom:updated>2024-04-10T05:07:15.208Z</atom:updated>
        </item>
        <item>
            <title>
                <![CDATA[ Design an Easy-to-Use Deep Learning Framework ]]>
            </title>
            <link>https://towardsdatascience.com/design-an-easy-to-use-deep-learning-framework-52d7c37e415f?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/52d7c37e415f</guid>
            <category>
                <![CDATA[ tips-and-tricks ]]>
            </category>
            <category>
                <![CDATA[ software-engineering ]]>
            </category>
            <category>
                <![CDATA[ deep-learning ]]>
            </category>
            <category>
                <![CDATA[ notes-from-industry ]]>
            </category>
            <category>
                <![CDATA[ deep-learning-framework ]]>
            </category>
            <dc:creator>
                <![CDATA[ Haifeng Jin ]]>
            </dc:creator>
            <pubDate>Wed, 10 Apr 2024 04:54:20 GMT</pubDate>
            <atom:updated>2024-04-10T04:54:20.400Z</atom:updated>
            <content:encoded>
                <![CDATA[ <h4>The three software design principles I learned as an open-source contributor</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KI_sWjoAdwTBGKh2" /><figcaption>Photo by <a href="https://unsplash.com/@hfestudio?utm_source=medium&amp;utm_medium=referral">Sheldon</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Deep learning frameworks are extremely transitory. If you compare the deep learning frameworks people use today with what it was eight years ago, you will find the landscape is completely different. There were Theano, Caffe2, and MXNet, which all went obsolete. Today&#39;s most popular frameworks, like TensorFlow and PyTorch, were just released to the public.</p><p>Through all these years, Keras has survived as a high-level user-facing library supporting different backends, including TensorFlow, PyTorch, and JAX. As a contributor to Keras, I learned how much the team cares about user experience for the software and how they ensured a good user experience by following a few simple yet powerful principles in their design process.</p><p>In this article, I will share the 3 most important software design principles I learned by contributing to the Keras through the past few years, which may be generalizable to all types of software and help you make an impact in the open-source community with yours.</p><h4>Why user experience is important for open-source software</h4><p>Before we dive into the main content, let’s quickly discuss why user experience is so important. We can learn this through the PyTorch vs. TensorFlow case.</p><p>They were developed by two tech giants, Meta and Google, and have quite different cultural strengths. Meta is good at product, while Google is good at engineering. As a result, Google’s frameworks like TensorFlow and JAX are the fastest to run and technically superior to PyTorch, as they support sparse tensors and distributed training well. However, PyTorch still took away half of the market share from TensorFlow because it prioritizes user experience over other aspects of the software.</p><p>Better user experience wins for the research scientists who build the models and propagate them to the engineers, who take models from them since they don’t always want to convert the models they receive from the research scientists to another framework. They will build new software around PyTorch to smooth their workflow, which will establish a software ecosystem around PyTorch.</p><p>TensorFlow also made a few blunders that caused its users to lose. TensorFlow’s general user experience is good. However, its installation guide for GPU support was broken for years before it was fixed in 2022. TensorFlow 2 broke the backward compatibility, which cost its users millions of dollars to migrate.</p><p>So, the lesson we learned here is that despite technical superiority, user experience decides which software the open-source users would choose.</p><h4>All deep learning frameworks invest heavily in user experience</h4><p>All the deep learning frameworks—TensorFlow, PyTorch, and JAX—invest heavily in user experience. Good evidence is that they all have a relatively high Python percentage in their codebases.</p><p>All the core logic of deep learning frameworks, including tensor operations, automatic differentiation, compilation, and distribution are implemented in C++. Why would they want to expose a set of Python APIs to the users? It is just because the users love Python and they want to polish their user experience.</p><h4>Investing in user experience is of high ROI</h4><p>Imagine how much engineering effort it requires to make your deep learning framework a little bit faster than others. A lot.</p><p>However, for a better user experience, as long as you follow a certain design process and some principles, you can achieve it. For attracting more users, your user experience is as important as the computing efficiency of your framework. So, investing in user experience is of high return on investment (ROI).</p><h4>The three principles</h4><p>I will share the three important software design principles I learned by contributing to Keras, each with good and bad code examples from different frameworks.</p><h4>Principle 1: Design end-to-end workflows</h4><p>When we think of designing the APIs of a piece of software, you may look like this.</p><pre>class Model:<br> def __call__(self, input):<br> &quot;&quot;&quot;The forward call of the model.<br><br> Args:<br> input: A tensor. The input to the model.<br> &quot;&quot;&quot;<br> pass</pre><p>Define the class and add the documentation. Now, we know all the class names, method names, and arguments. However, this would not help us understand much about the user experience.</p><p>What we should do is something like this.</p><pre>input = keras.Input(shape=(10,))<br>x = layers.Dense(32, activation=&#39;relu&#39;)(input)<br>output = layers.Dense(10, activation=&#39;softmax&#39;)(x)<br>model = keras.models.Model(inputs=input, outputs=output)<br>model.compile(<br> optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;<br>)</pre><p>We want to write out the entire user workflow of using the software. Ideally, it should be a tutorial on how to use the software. It provides much more information about the user experience. It may help us spot many more UX problems during the design phase compared with just writing out the class and methods.</p><p>Let’s look at another example. This is how I discovered a user experience problem by following this principle when implementing KerasTuner.</p><p>When using KerasTuner, users can use this RandomSearch class to select the best model. We have the metrics, and objectives in the arguments. By default, objective equals validation loss. So, it helps us find the model with the smallest validation loss.</p><pre>class RandomSearch:<br> def __init__(self, ..., metrics, objective=&quot;val_loss&quot;, ...):<br> &quot;&quot;&quot;The initializer.<br><br> Args:<br> metrics: A list of Keras metrics.<br> objective: String or a custom metric function. The<br> name of the metirc we want to minimize.<br> &quot;&quot;&quot;<br> pass</pre><p>Again, it doesn’t provide much information about the user experience. So, everything looks OK for now.</p><p>However, if we write an end-to-end workflow like the following. It exposes many more problems. The user is trying to define a custom metric function named custom_metric. The objective is not so straightforward to use anymore. What should we pass to the objective argument now?</p><pre>tuner = RandomSearch(<br> ...,<br> metrics=[custom_metric],<br> objective=&quot;val_???&quot;,<br>)</pre><p>It should be just &quot;val_custom_metric”. Just use the prefix of &quot;val_&quot; and the name of the metric function. It is not intuitive enough. We want to make it better instead of forcing the user to learn this. We easily spotted a user experience problem by writing this workflow.</p><p>If you wrote the design more comprehensively by including the implementation of the custom_metric function, you will find you even need to learn how to write a Keras custom metric. You have to follow the function signature to make it work, as shown in the following code snippet.</p><pre>def custom_metric(y_true, y_pred):<br> squared_diff = ops.square(y_true - y_pred)<br> return ops.mean(squared_diff, axis=-1)</pre><p>After discovering this problem. We specially designed a better workflow for custom metrics. You only need to override HyperModel.fit() to compute your custom metric and return it. No strings to name the objective. No function signature to follow. Just a return value. The user experience is much better right now.</p><pre>class MyHyperModel(HyperModel):<br> def fit(self, trial, model, validation_data):<br> x_val, y_true = validation_data<br> y_pred = model(x_val)<br> return custom_metric(y_true, y_pred)<br><br>tuner = RandomSearch(MyHyperModel(), max_trials=20)</pre><p>One more thing to remember is we should always start from the user experience. The designed workflows backpropagate to the implementation.</p><h4>Principle 2: Minimize cognitive load</h4><p>Do not force the user to learn anything unless it is really necessary. Let’s see some good examples.</p><p>The Keras modeling API is a good example shown in the following code snippet. The model builders already have these concepts in mind, for example, a model is a stack of layers. It needs a loss function. We can fit it with data or make it predict on data.</p><pre>model = keras.Sequential([<br> layers.Dense(10, activation=&quot;relu&quot;),<br> layers.Dense(num_classes, activation=&quot;softmax&quot;),<br>])<br>model.compile(loss=&#39;categorical_crossentropy&#39;)<br>model.fit(...)<br>model.predict(...)</pre><p>So basically, no new concepts were learned to use Keras.</p><p>Another good example is the PyTorch modeling. The code is executed just like Python code. All tensors are just real tensors with real values. You can depend on the value of a tensor to decide your path with plain Python code.</p><pre>class MyModel(nn.Module):<br> def forward(self, x):<br> if x.sum() &gt; 0:<br> return self.path_a(x)<br> return self.path_b(x)</pre><p>You can also do this with Keras with TensorFlow or JAX backend but needs to be written differently. All the if conditions need to be written with this ops.cond function as shown in the following code snippet.</p><pre>class MyModel(keras.Model):<br> def call(self, inputs):<br> return ops.cond(<br> ops.sum(inputs) &gt; 0,<br> lambda : self.path_a(inputs),<br> lambda : self.path_b(inputs),<br> )</pre><p>This is teaching the user to learn a new op instead of using the if-else clause they are familiar with, which is bad. In compensation, it brings significant improvement in training speed.</p><p>Here is the catch of the flexibility of PyTorch. If you ever needed to optimize the memory and speed of your model, you would have to do it by yourself using the following APIs and new concepts to do so, including the inplace arguments for the ops, the parallel op APIs, and explicit device placement. It introduces a rather high learning curve for the users.</p><pre>torch.relu(x, inplace=True)<br>x = torch._foreach_add(x, y)<br>torch._foreach_add_(x, y)<br>x = x.cuda()</pre><p>Some other good examples are keras.ops, tensorflow.numpy, jax.numpy. They are just a reimplementation of the numpy API. When introducing some cognitive load, just reuse what people already know. Every framework has to provide some low-level ops in these frameworks. Instead of letting people learn a new set of APIs, which may have a hundred functions, they just use the most popular existing API for it. The numpy APIs are well-documented and have tons of Stack Overflow questions and answers related to it.</p><p>The worst thing you can do with user experience is to trick the users. Trick the user to believe your API is something they are familiar with but it is not. I will give two examples. One is on PyTorch. The other one is on TensorFlow.</p><p>What should we pass as the pad argument in F.pad() function if you want to pad the input tensor of the shape (100, 3, 32, 32) to (100, 3, 1+32+1, 2+32+2) or (100, 3, 34, 36)?</p><pre>import torch.nn.functional as F<br># pad the 32x32 images to (1+32+1)x(2+32+2)<br># (100, 3, 32, 32) to (100, 3, 34, 36)<br>out = F.pad(<br> torch.empty(100, 3, 32, 32),<br> pad=???,<br>)</pre><p>My first intuition is that it should be ((0, 0), (0, 0), (1, 1), (2, 2)), where each sub-tuple corresponds to one of the 4 dimensions, and the two numbers are the padding size before and after the existing values. My guess is originated from the numpy API.</p><p>However, the correct answer is (2, 2, 1, 1). There is no sub-tuple, but one plain tuple. Moreover, the dimensions are reversed. The last dimension goes the first.</p><p>The following is a bad example from TensorFlow. Can you guess what is the output of the following code snippet?</p><pre>value = True<br><br>@tf.function<br>def get_value():<br> return value<br><br>value = False<br>print(get_value())</pre><p>Without the tf.function decorator, the output should be False, which is pretty simple. However, with the decorator, the output is True. This is because TensorFlow compiles the function and any Python variable is compiled into a new constant. Changing the old variable’s value would not affect the created constant.</p><p>It tricks the user into believing it is the Python code they are familiar with, but actually, it is not.</p><h4>Principle 3: Interaction over documentation</h4><p>No one likes to read long documentation if they can figure it out just by running some example code and tweaking it by themselves. So, we try to make the user workflow of the software follow the same logic.</p><p>Here is a good example shown in the following code snippet. In PyTorch, all methods with the underscore are inplace ops, while the ones without are not. From an interactive perspective, these are good, because they are easy to follow, and the users do not need to check the docs whenever they want the inplace version of a method. However, of course, they introduced some cognitive load. The users need to know what does inplace means and when to use them.</p><pre>x = x.add(y)<br>x.add_(y)<br>x = x.mul(y)<br>x.mul_(y)</pre><p>Another good example is the Keras layers. They strictly follow the same naming convention as shown in the following code snippet. With a clear naming convention, the users can easily remember the layer names without checking the documentation.</p><pre>from keras import layers<br><br>layers.MaxPooling2D()<br>layers.GlobalMaxPooling1D()<br>layers.GlobalAveragePooling3D()</pre><p>Another important part of the interaction between the user and the software is the error message. You cannot expect the user to write everything correctly the very first time. We should always do the necessary checks in the code and try to print helpful error messages.</p><p>Let’s see the following two examples shown in the code snippet. The first one has not much information. It just says tensor shape mismatch. The<br> second one contains much more useful information for the user to find the bug. It not only tells you the error is because of tensor shape mismatch, but it also shows what is the expected shape and what is the wrong shape it received. If you did not mean to pass that shape, you have a better idea<br> of the bug now.</p><pre># Bad example:<br>raise ValueError(&quot;Tensor shape mismatch.&quot;)<br><br># Good example:<br>raise ValueError(<br> &quot;Tensor shape mismatch. &quot;<br> &quot;Expected: (batch, num_features). &quot;<br> f&quot;Received: {x.shape}&quot;<br>)</pre><p>The best error message would be directly pointing the user to the fix. The following code snippet shows a general Python error message. It guessed what was wrong with the code and directly pointed the user to the fix.</p><pre>import math<br><br>math.sqr(4)<br>&quot;AttributeError: module &#39;math&#39; has no attribute &#39;sqr&#39;. Did you mean: &#39;sqrt&#39;?&quot;</pre><h4>Final words</h4><p>So far we have introduced the three most valuable software design principles I have learned when contributing to the deep learning frameworks. First, write end-to-end workflows to discover more user experience problems. Second, reduce cognitive load and do not teach the user anything unless necessary. Third, follow the same logic in your API design and throw meaningful error messages so that the users can learn your software by interacting with it instead of constantly checking the documentation.</p><p>However, there are many more principles to follow if you want to make your software even better. You can refer to the <a href="https://github.com/keras-team/governance/blob/24401c1addf521e522fd363f6eb40e7c4c4881d5/keras_api_design_guidelines.md">Keras API design guidelines</a> as a complete API design guide.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=52d7c37e415f" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/design-an-easy-to-use-deep-learning-framework-52d7c37e415f">Design an Easy-to-Use Deep Learning Framework</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
            </content:encoded>
        </item>
        <item>
            <title>
                <![CDATA[ Linear Regressions for Causal Conclusions ]]>
            </title>
            <link>https://towardsdatascience.com/linear-regressions-for-causal-conclusions-34c6317c5a11?source=rss----7f60cf5620c9---4</link>
            <guid isPermaLink="false">https://medium.com/p/34c6317c5a11</guid>
            <category>
                <![CDATA[ getting-started ]]>
            </category>
            <category>
                <![CDATA[ ab-testing ]]>
            </category>
            <category>
                <![CDATA[ linear-regression ]]>
            </category>
            <category>
                <![CDATA[ data-science ]]>
            </category>
            <category>
                <![CDATA[ causal-inference ]]>
            </category>
            <dc:creator>
                <![CDATA[ Mariya Mansurova ]]>
            </dc:creator>
            <pubDate>Wed, 10 Apr 2024 04:45:23 GMT</pubDate>
            <atom:updated>2024-04-10T04:45:23.778Z</atom:updated>
            <content:encoded>
                <![CDATA[ <h4>An easy and yet powerful tool for decision making</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XbcjlXRUFSs3K78hcs8_wg.jpeg" /><figcaption>Image by DALL-E</figcaption></figure><p>I suppose most of us have heard the statement “correlation doesn’t imply causation” multiple times. It often becomes a problem for analysts since we frequently can see only correlations but still want to make causal conclusions.</p><p>Let’s discuss a couple of examples to understand the difference better. I would like to start with a case from everyday life rather than the digital world.</p><p>In 1975, a vast population study was launched in Denmark. It’s called <a href="https://pubmed.ncbi.nlm.nih.gov/30193744/">the Copenhagen City Heart Study (CCHS)</a>. Researchers gathered information about 20K men and women and have been monitoring these people for decades. The initial goal of this research was to find ways to prevent cardiovascular diseases and strokes. One of the conclusions from this study is that people who reported regularly playing tennis have 9.7 years higher life expectancy.</p><p>Let’s think about how we could interpret this information. Does it mean that if a person starts playing tennis weekly today, they will increase their life expectancy by ten years? Unfortunately, not exactly. Since it’s an observational study, we should be cautious about making causal inferences. There might be some other effects. For example, tennis players are likely to be wealthier, and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4866586/">we know</a> that higher wealth correlates with greater longevity. Or there could be a correlation that people who regularly do sports also care more about their health and, because of it, do all checkups regularly. So, observational research might overestimate the effect of tennis on longevity since it doesn’t control other factors.</p><p>Let’s move on to the examples closer to product analytics and our day-to-day job. The number of Customer Support contacts for a client will likely be positively correlated with the probability of churn. If customers had to contact our support ten times, they would likely be annoyed and stop using our product, while customers who never had problems and are happy with the service might never reach out with any questions.</p><p>Does it mean that if we reduce the number of CS contacts, we will increase customer retention? I’m ready to bet that if we hide contact info and significantly reduce the number of CS contacts, we won’t be able to decrease churn because the actual root cause of churn is not CS contact but customers’ dissatisfaction with the product, which leads to both customers contacting us and stopping using our product.</p><p>I hope that with these examples, you can gain some intuition about the correlation vs. causation problem.</p><p>In this article, I would like to share approaches for driving causal conclusions from the data. Surprisingly, we will be able to use the most basic tool — just a linear regression.</p><p>If we use the same linear regression for causal inference, you might wonder, what is the difference between our usual approach and causal analytics? That’s a good question. Let’s start our causal journey by understanding the differences between approaches.</p><h3>Predictive vs. causal analytics</h3><p>Predictive analytics helps to make forecasts and answer questions like “How many customers will we have in a year if nothing changes?” or “What is the probability for this customer to make a purchase within the next seven days?”.</p><p>Causal analytics tries to understand the root causes of the process. It might help you to answer “what if” questions like “How many customers will churn if we increase our subscription fee?” or “How many customers would have signed up for our subscription if we didn’t launch this Saint Valentine’s promo?”.</p><p>Causal questions seem way more complicated than just predictive ones. However, these two approaches often leverage the same tools, such as linear or logistic regressions. Even though tools are the same, they have absolutely different goals:</p><ul><li>For predictive analytics, we try our best to predict a value in the future based on information we know. So, the main KPI is an error in the prediction.</li><li>Building a regression model for the causal analysis, we focus on the relationships between our target value and other factors. The model’s main output is coefficients rather than forecasts.</li></ul><p>Let’s look at a simple example. Suppose we would like to forecast the number of active customers.</p><ul><li>In the predictive approach, we are talking about baseline forecast (given that the situation will stay pretty much the same). We can use <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjoheb60ZeEAxVkyQIHHY0JALwQFnoECCgQAQ&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAutoregressive_integrated_moving_average&amp;usg=AOvVaw3LwrU_FST2Kj6NbHUsUNNT&amp;opi=89978449">ARIMA</a> (<em>Autoregressive Integrated Moving Average</em>) and base our projections on previous values. ARIMA works well for predictions but can’t tell you anything about the factors affecting your KPI and how to improve your product.</li><li>In the case of causal analytics, our goal is to find causal relationships in our data, so we will build a regression and identify factors that can impact our KPI, such as subscription fees, marketing campaigns, seasonality, etc. In that case, we will get not only the BAU (business as usual) forecast but also be able to estimate different “what if” scenarios for the future.</li></ul><p>Now, it’s time to dive into causal theory and learn basic terms.</p><h3>Correlation doesn’t imply causation</h3><p>Let’s consider the following example for our discussion. Imagine you sent a discount coupon to loyal customers of your product, and now you want to understand how it affected their value (money spent on the product) and retention.</p><p>One of the most basic causal terms is <strong>treatment</strong>. It sounds like something related to the medicine, but actually, it’s just an intervention. In our case, it’s a discount. We usually define treatment at the unit level (in our case, customer) in the following way.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8Xg3eyCOaeZE0lAQQEgxAA.png" /></figure><p>The other crucial term is <strong>outcome</strong> Y, our variable of interest. In our example, it’s the customer’s value.</p><p>The fundamental problem of causal inference is that we can’t observe both outcomes for the same customers. So, if a customer received the discount, we will never know what value or retention he would have had without a coupon. It makes causal inference tricky.</p><p>That’s why we need to introduce another concept — <strong>potential outcomes</strong>. The outcome that happened is usually called factual, and the one that didn’t is counterfactual. We will use the following notation for it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1qZwhDr5Y9SvGYhpI6hjVg.png" /></figure><p>The main goal of causal analysis is to measure the relationship between treatment and outcome. We can use the following metrics to quantify it:</p><ul><li><strong>ATE</strong> — average treatment effect,</li><li><strong>ATT</strong> — average treatment effect on treated (customers with the treatment)</li></ul><p>They are both equal to <a href="https://en.wikipedia.org/wiki/Expected_value">expected values</a> of the differences between potential outcomes either for all units (customers in our case) or only for treated ones.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YvmMLyfSphkppu3IK7O8-g.png" /></figure><p>That’s an actual causal effect, and unfortunately, we won’t be able to calculate it. But cheer up; we can still get some estimations. We can observe the difference between values for treated and not treated customers (correlation effect). Let’s try to interpret this value.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*id9AcuCVH5qD7LDxi2l7ZA.png" /></figure><p>Using a couple of simple mathematical transformations (i.e. adding and subtracting the same value), we’ve concluded that the average in values between treated and not treated customers equals the sum of <strong>ATT</strong> (average treatment effect on treated) and <strong>bias</strong> term. The bias equals the difference between control and treatment groups without a treatment.</p><p>If we return to our case, the bias will be equal to the difference between expected customer value for the treatment group if they haven’t received discount (<em>counterfactual outcome</em>) and the control group (<em>factual outcome</em>).</p><p>In our example, the average value from customers who received a discount will likely be much higher than for those who didn’t. Could we attribute all this effect to our treatment (discount coupon)? Unfortunately not. Since we sent discount to loyal customers who are already spending a lot of money in our product, they would likely have higher value than control group even without a treatment. So, there’s a bias, and we can’t say that the difference in value between two segments equals ATT.</p><p>Let’s think about how to overcome this obstacle. We can do an A/B test: randomly split our loyal customers into two groups and send discount coupons only to half of them. Then, we can estimate the discount’s effect as the average difference between these two groups since we’ve eliminated bias (without treatment, there’s no difference between these groups except for discount).</p><p>We’ve covered the basic theory of causal inference and have learned the most crucial concept of bias. So, we are ready to move on to practice. We will start by analysing the A/B test results.</p><h3>Use case: A/B test</h3><p>Randomised controlled trial (RTC), often called the A/B test, is a powerful tool for getting causal conclusions from data. This approach assumes that we are assigning treatment randomly, and it helps us eliminate bias (since groups are equal without treatment).</p><p>To practice solving such tasks, we will look at the example based on synthetic data. Suppose we’ve built an LLM-based tool that helps customer support agents answer questions more quickly. To measure the effect, we introduced this tool to half of the agents, and we would like to measure how our treatment (LLM-based tool) affects the outcome (time the agent spends answering a customer’s question).</p><p>Let’s have a quick look at the data we have.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J67bN8OtiL0iARPLh_Rn2w.png" /></figure><p>Here are the description of the parameters we logged:</p><ul><li>case_id — unique ID for the case.</li><li>agent_id — unique ID for the agent.</li><li>treatment equals 1 if agent was in an experiment group and have a chance to use LLMs, 0 — otherwise.</li><li>time_spent_mins — minutes spent answering the customer’s question.</li><li>cs_center — customer support centre. We are working with several CS centres. We launched this experiment in some of them because it’s easier to implement. Such an approach also helps us to avoid contamination (when agents from experiment and control groups interact and can affect each other).</li><li>complexity equals low, medium or high. This feature is based on the category of the customer’s question and defines how much time an agent is supposed to spend solving this case.</li><li>tenure — number of months since the agent started working.</li><li>passed_training — whether the agent passed LLM training. This value can be equal to True only for the treatment group since this training wasn’t offered to the agents from the control group.</li><li>within_sla equals 1 if the agent was able to answer the question within SLA (15 minutes).</li></ul><p>As usual, let’s start with a high-level overview of the data. We have quite a lot of data points, so we will likely be able to get statistically significant results. Also, we can see way lower average response times for the treatment group, so we can hope that the LLM tool really helps.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ElUR0QgtEwEcKCnwg_MI1A.png" /></figure><p>I also usually look at the actual distributions since average statistics might be misleading. In this case, we can see two unimodal distributions without distinctive outliers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*79vsGZCaB4X6FrgRX0GDNQ.png" /><figcaption>Image by author</figcaption></figure><h4>Classic statistical approach</h4><p>The classic approach to analysing A/B tests is to use statistical formulas. Using <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html">the scipy package</a>, we can calculate the confidence interval for the difference between the two means.</p><pre># defining samples<br>control_values = df[df.treatment == 0].time_spent_mins.values<br>exp_values = df[df.treatment == 1].time_spent_mins.values<br><br># calculating p-values<br>from scipy.stats import ttest_ind<br><br>ttest_ind(exp_values, control_values)<br># Output: TtestResult(statistic=-70.2769283935386, pvalue=0.0, df=89742.0)</pre><p>We got a p-value below 1%. So, we can reject the null hypothesis and conclude that there’s a difference in average time spent per case in the control and test groups. To understand the effect size, we can also calculate the confidence interval.</p><pre>from scipy import stats<br>import numpy as np<br><br># Calculate sample statistics<br>mean1, mean2 = np.mean(exp_values), np.mean(control_values)<br>std1, std2 = np.std(exp_values, ddof=1), np.std(control_values, ddof=1)<br>n1, n2 = len(exp_values), len(control_values)<br>pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))<br>degrees_of_freedom = n1 + n2 - 2<br>confidence_level = 0.95<br><br># Calculate margin of error<br>margin_of_error = stats.t.ppf((1 + confidence_level) / 2, degrees_of_freedom) * pooled_std * np.sqrt(1 / n1 + 1 / n2)<br><br># Calculate confidence interval<br>mean_difference = mean1 - mean2<br>conf_interval = (mean_difference - margin_of_error, <br> mean_difference + margin_of_error)<br><br>print(&quot;Confidence Interval:&quot;, list(map(lambda x: round(x, 3), conf_interval)))<br># Output: Confidence Interval: [-1.918, -1.814]</pre><p>As expected since p-value is below 5%, our confidence interval doesn’t include 0.</p><p>The traditional approach works. However, we can get the same results with linear regression, which will also allow us to do more advanced analysis later. So, let’s discuss this method.</p><h4>Linear regression basics</h4><p>As we already discussed, observing both potential outcomes (with and without treatment) for the same object is impossible. Since we won’t be able to estimate the impact on each object individually, we need a model. Let’s assume the constant treatment effect.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MNvAypGvzqef7HEWIQAnjA.png" /></figure><p>Then, we can write down the relation between outcome (time spent on request) and treatment in the following way, where</p><ul><li>baseline is a constant that shows the basic level of outcome,</li><li>residual represents other potential relationships we don’t care about right now (for example, the agent’s maturity or complexity of the case).</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*19xo3bqHPGAgBUtSQjxIDg.png" /></figure><p>It’s a linear equation, and we can get the estimation of the impact variable using linear regression. We will use <a href="https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html">OLS (Ordinary Least Squares)</a> function from statsmodels package.</p><pre>import statsmodels.formula.api as smf<br>model = smf.ols(&#39;time_spent_mins ~ treatment&#39;, data=df).fit()<br>model.summary().tables[1]</pre><p>In the result, we got all the needed info: estimation of the effect (coefficient for the treatment variable), its p-value and confidence interval.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cTEVKIorKPlOnGY8Ri7rXA.png" /></figure><p>Since the p-value is negligible (definitely below 1%), we can consider the effect significant and say that our LLM-based tool helps to reduce the time spent on a case by 1.866 minutes with a 95% confidence interval (1.814, 1.918). You can notice that we got exactly the same result as with statistical formulas before.</p><h4>Adding more variables</h4><p>As promised, we can make a more complex analysis with linear regression and take into account more factors, so let’s do it. In our initial approach, we used only one regressor — treatment flag. However, we can add more variables (for example, complexity).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7ObO1NyX8Pp7yp_A1megOA.png" /></figure><p>In this case, the impact will show estimation after accounting for all the effects of other variables in the model (in our case — task complexity). Let’s estimate it. Adding more variables into the regression model is straightforward — we just need to add another component to the equation.</p><pre>import statsmodels.formula.api as smf<br>model = smf.ols(&#39;time_spent_mins ~ treatment + complexity&#39;, data=df).fit()<br>model.summary().tables[1]</pre><p>Now, we see a bit higher estimation of the effect — 1.91 vs 1.87 minutes. Also, the error has decreased (0.015 vs 0.027), and the confidence interval has narrowed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*z-JS8KDgvQy3YKPPCRIRUA.png" /></figure><p>You can also notice that since complexity is a categorical variable, it was automatically converted into a bunch of dummy variables. So, we got estimations of -9.8 minutes for low-complexity tasks and -4.7 minutes for medium ones.</p><p>Let’s try to understand why we got a more confident result after adding complexity. Time spent on a customer case significantly depends on the complexity of the tasks. So, complexity is responsible for a significant amount of our variable’s variability.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hrANeJzeylbGkvBjQg8NRQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6EjQMebqGJJuJRBpuBmRxw.png" /><figcaption>Image by author</figcaption></figure><p>As I mentioned before, the coefficient for treatment estimates the impact after accounting for all the other factors in the equation. When we added complexity to our linear regression, it reduced the variance of residuals, and that’s why we got a narrower confidence interval for time.</p><p>Let’s double-check that complexity explains a significant proportion of variance. We can see a considerable decrease: time spent has a variance equal to 16.6, but when we account for complexity, it reduces to just 5.9.</p><pre>time_model = smf.ols(&#39;time_spent_mins ~ complexity&#39;, data=df).fit()<br><br>print(&#39;Initial variance: %.2f&#39; % (df.time_spent_mins.var()))<br>print(&#39;Residual variance after accounting for complexity: %.2f&#39; \<br> % (time_model.resid.var()))<br><br># Output: <br># Initial variance: 16.63<br># Residual variance after accounting for complexity: 5.94</pre><p>So, we can see that adding a factor that can predict the outcome variable to a linear regression can improve your effect size estimations. Also, it’s worth noting that the variable is not correlated with treatment assignment (the tasks of each complexity have equal chances to be in the control or test group).</p><p>Traditionally, causal graphs are used to show the relationships between the variables. Let’s draw such a graph to represent our current situation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*N29E4akuek2VhEZKzQZbAw.png" /><figcaption>Image by author</figcaption></figure><h4>Non-linear relationships</h4><p>So far, we’ve looked only at linear relationships, but sometimes, it’s not enough to model our situation.</p><p>Let’s look at the data on LLM training that agents from the experiment group were supposed to pass. Only half of them have passed the LLM training and learned how to use the new tool effectively.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*r7lBtaGfWXqXqTKNT0KdWw.png" /></figure><p>We can see a significant difference in average time spent for the treatment group who passed training vs. those who didn’t.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ks55hd6dFDqSkoWy8XTO5Q.png" /><figcaption>Image by author</figcaption></figure><p>So, we should expect different impacts from treatment for these two groups. We can use non-linearity to express such relationships in formulas and add treatment * passed_training component to our equation.</p><pre>model = smf.ols(&#39;time_spent_mins ~ treatment * passed_training + complexity&#39;, <br> data=df).fit()<br>model.summary().tables[1]</pre><p>The treatment and passed_training factors will also be automatically added to the regression. So, we will be optimising the following formula.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AJC10v9ayFASAAD1n5ECIQ.png" /></figure><p>We got the following results from the linear regression.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0LC42lgWIAzzvFTIxjwGBA.png" /></figure><p>No statistically significant effect is associated with passed training since the p-value is above 5%, while other coefficients differ from zero.</p><p>Let’s put down all the different scenarios and estimate the effects using the coefficients we got from the linear regression.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uHezct6jlrGNkOlWHxfFbA.png" /></figure><p>So, we’ve got new treatment estimations: 2.5 minutes improvement per case for the agents who have passed the training and 1.3 minutes — for those who didn’t.</p><h4>Confounders</h4><p>Before jumping to conclusions, it’s worth double-checking some assumptions we made — for example, random assignment. We’ve discussed that we launched the experiment in some CS centres. Let’s check whether agents in the different centres are similar so that our control and test groups are non-biased.</p><p>We know that agents differ by experience, which might significantly affect their performance. Our day-to-day intuition tells us that more experienced agents will spend less time on tasks. We can see in the data that it is actually like this.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CxAFgSePZkcUrYFerY2HuQ.png" /><figcaption>Image by author</figcaption></figure><p>Let’s see whether our experiment and control have the same level of agents’ experience. The easiest way to do it is to look at distributions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n9nwIacThexPvGPxVXPQag.png" /><figcaption>Image by author</figcaption></figure><p>Apparently, agents in the treatment group have much more experience than the ones in the control group. Overall, it makes sense that the product team decided to launch the experiment, starting with the more trained agents. However, it breaks our assumption about random assignment. Since the control and test groups are different even without treatment, we are overestimating the effect of our LLM tool on the agents’ performance.</p><p>Let’s return to our causal graph. The agent’s experience affects both treatment assignment and output variable (time spent). Such variables are called confounders.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JN0AfiyATIJf6J-Oc8EyUA.png" /><figcaption>Image by author</figcaption></figure><p>Don’t worry. We can solve this issue effortlessly — we just need to include confounders in our equation to control for it. When we add it to the linear regression, we start to estimate the treatment effect with fixed experience, eliminating the bias. Let’s try to do it.</p><pre>model = smf.ols(&#39;time_spent_mins ~ treatment * passed_training + complexity + tenure&#39;, data=df).fit()<br>model.summary().tables[1]</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tsMkR4agbzRRoKEsrzx9Hw.png" /></figure><p>With added tenure, we got the following results:</p><ul><li>There is no statistically significant effect of passed training or treatment alone since the p-value is above 5%. So, we can conclude that an LLM helper does not affect agents’ performance unless they have passed the training. In the previous iteration, we saw a statistically significant effect, but it was due to tenure confounding bias.</li><li>The only statistically significant effect is for the treatment group with passed training. It equals 1.07 minutes with a 95% confidence interval (1.02, 1.11).</li><li>Each month of tenure is associated with 0.05 minutes less time spent on the task.</li></ul><p>We are working with synthetic data so we can easily compare our estimations with actual effects. The LLM tool reduces the time spent per task by 1 minute if the agent has passed the training, so our estimations are pretty accurate.</p><h4>Bad controls</h4><p>Machine learning tasks are often straightforward: you gather data with all possible features you can get, try to fit some models, compare their performance and pick the best one. Contrarily, causal inference requires some art and a deep understanding of the process you’re working with. One of the essential questions is what features are worth including in regression and which ones will spoil your results.</p><p>Till now, all the additional variables we’ve added to the linear regression have been improving the accuracy. So, you might think adding all your features to regression will be the best strategy. Unfortunately, it’s not that easy for causal inference. In this section, we will look at a couple of cases when additional variables decrease the accuracy of our estimations.</p><p>For example, we have a CS centre in data. We’ve assigned treatment based on the CS centre, so including it in the regression might sound reasonable. Let’s try.</p><pre>model = smf.ols(&#39;time_spent_mins ~ treatment + complexity + tenure + cs_center&#39;, <br> data=df[df.treatment == df.passed_training]).fit()<br>model.summary().tables[1]</pre><p>For simplicity, I’ve removed non-linearity from our dataset and equation, filtering out cases where the agents from the treatment groups didn’t pass the LLM training.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MCGkDhz7H-RLunmz6EmAEw.png" /></figure><p>If we include the CS centre in linear regression, we will get a ridiculously high estimation of the effect (around billions) without statistical significance. So, this variable is rather harmful than helpful.</p><p>Let’s update a causal chart and try to understand why it doesn’t work. CS centre is a predictor for our treatment but has no relationship with the output variable (so it’s not a confounder). Adding a treatment predictor leads to <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a> (like in our case) or reduces the treatment variance (it’s challenging to estimate the effect of treatment on the output variable since treatment doesn’t change much). So, it’s a bad practice to add such variables to the equation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QOKV4dqvY7xlO4uqX2DRIQ.png" /><figcaption>Image by author</figcaption></figure><p>Let’s move on to another example. We have a within_sla variable showing whether the agents finished the task within 15 minutes. Can this variable improve the quality of our effect estimations? Let’s see.</p><pre>model = smf.ols(&#39;time_spent_mins ~ treatment + complexity + tenure + within_sla&#39;, <br> data=df[df.treatment == df.passed_training]).fit()<br>model.summary().tables[1]</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NwI4GGKmMRk_MwrhZ8rgrQ.png" /></figure><p>The new effect estimation is way lower: 0.8 vs 1.1 minutes. So, it poses a question: which one is more accurate? We’ve added more parameters to this model, so it’s more complex. Should it give more precise results, then? Unfortunately, it’s not always like that. Let’s dig deeper into it.</p><p>In this case, within_sla flag shows whether the agent solved the problem within 15 minutes or the question took more time. So, if we return to our causal chart, within_sla flag is an outcome of our output variable (time spent on the task).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KTiP7SHeiyer9vFA5s3EMw.png" /><figcaption>Image by author</figcaption></figure><p>When we add the within_slag flag into regression and control for it, we are starting to estimate the effect of treatment with a fixed value of within_sla. So, we will have two cases: within_sla = 1 and within_sla = 0. Let’s look at the bias for each of them.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XcsaKoP3V8GNnPgc4LC7ig.png" /></figure><p>In both cases, bias is not equal to 0, which means our estimation is biased. At first glance, it might look a bit counterintuitive. Let me explain the logic behind it a bit.</p><ul><li>In the first equation, we compare cases where agents finished the tasks within 15 minutes with the help of the LLM tool and without. The previous analysis shows that the LLM tool (our treatment) tends to speed up agents’ work. So, if we compare the expected time spent on tasks without treatments (when agents work independently without the LLM tool), we should expect quicker responses from the second group.</li><li>Similarly, for the second equation, we are comparing agents who haven’t completed tasks within 15 minutes, even with the help of LLM and those who did it on their own. Again, we should expect longer response times from the first group without treatment.</li></ul><p>It’s an example of selection bias — a case when we control for a variable on the path from treatment to output variable or outcome of the output variable. Controlling for such variables in a linear regression also leads to biased estimations, so don’t do it.</p><h4>Grouped data</h4><p>In some cases, you might not have granular data. In our example, we might not know the time spent on each task individually, but know the averages. It’s easier to track aggregated numbers for agents. For example, “within two hours, an agent closed 15 medium tasks”. We can aggregate our raw data to get such statistics.</p><pre>agents_df = df.groupby([&#39;agent_id&#39;, &#39;treatment&#39;, &#39;complexity&#39;, &#39;tenure&#39;, <br> &#39;passed_training&#39;], as_index = False).aggregate(<br> {&#39;case_id&#39;: &#39;nunique&#39;, &#39;time_spent_mins&#39;: &#39;mean&#39;}<br>)</pre><p>It’s not a problem for linear regression to deal with agent-level data. We just need to specify weights for each agent (equal to the number of cases).</p><pre><br>model = smf.ols(&#39;time_spent_mins ~ treatment + complexity + tenure&#39;, <br> data = agents_df[agents_df.treatment == agents_df.passed_training],<br> weights = agents_df[agents_df.treatment == agents_df.passed_training][&#39;case_id&#39;])\<br> .fit()<br>model.summary().tables[1]</pre><p>With aggregated data, we have roughly the same results for the effect of treatment. So, there’s no problem if you have only average numbers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*h7avIR8k81v7toMh-D601A.png" /></figure><h3>Use case: observational data</h3><p>We’ve looked at the A/B test examples for causal inference in detail. However, in many cases, we can’t conduct a proper randomised trial. Here are some examples:</p><ul><li>Some experiments are unethical. For example, you can’t push students to drink alcohol or smoke to see how it affects their performance at university.</li><li>In some cases, you might be unable to conduct an A/B test because of legal limitations. For example, you can’t charge different prices for the same product.</li><li>Sometimes, it’s just impossible. For example, if you are working on an extensive rebranding, you will have to launch it globally one day with a big PR announcement.</li></ul><p>In such cases, you have to use just observations to make conclusions. Let’s see how our approach works in such a case. We will use the <a href="https://archive.ics.uci.edu/dataset/320/student+performance">Student Performance data set</a> from the UC Irvine Machine Learning Repository.</p><p>Let’s use this real-life data to investigate how willingness to take higher education affects the math class’s final score. We will start with a trivial model and a causal chart.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NsleKrwmhkS1zxgckenhnA.png" /><figcaption>Image by author</figcaption></figure><pre>df = pd.read_csv(&#39;student-mat.csv&#39;, sep = &#39;;&#39;)<br>model = smf.ols(&#39;G3 ~ higher&#39;, data=df).fit()<br>model.summary().tables[1]</pre><p>We can see that willingness to continue education statistically significantly increases the final grade for the course by 3.8 points.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LxIdeHirPYDgneynFZRYlA.png" /></figure><p>However, there might be some confounders that we have to control for. For example, parents’ education can affect both treatments (<em>children are more likely to plan to take higher education if their parents have it</em>) and outcomes (<em>educated parents are more likely to help their children so that they have higher grades</em>). Let’s add the mother and father’s education level to the model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*w6SCCjAK9h5GBwB3SjCwoQ.png" /><figcaption>Image by author</figcaption></figure><pre>model = smf.ols(&#39;G3 ~ higher + Medu + Fedu&#39;, data=df).fit()<br>model.summary().tables[1]</pre><p>We can see a statistically significant effect from the mother’s education. We likely improved the accuracy of our estimation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uQdEYpOdls-p6AsC6k4bcA.png" /></figure><p>However, we should treat any causal conclusions based on observational data with a pinch of salt. We can’t be sure that we’ve taken into account all confounders and that the estimation we’ve got is entirely unbiased.</p><p>Also, it might be tricky to interpret the direction of the relation. We are sure there’s a correlation between willingness to continue education and final grade. However, we can interpret it in multiple ways:</p><ul><li>Students who want to continue their education are more motivated, so they have higher final grades.</li><li>Students with higher final grades are inspired by their success in studying, and that’s why they want to continue their education.</li></ul><p>With observational data, we can only use our common sense to choose one option or the other. There’s no way to infer this conclusion from data.</p><p>Despite the limitations, we can still use this tool to try our best to come to some conclusions about the world. As I mentioned, causal inference is based significantly on domain knowledge and common sense, so it’s worth spending time near the whiteboard to think deeply about the process you’re modelling. It will help you to achieve excellent results.</p><blockquote>You can find complete code for these examples on <a href="https://github.com/miptgirl/miptgirl_medium/tree/main/causal_inference_linear_regression">GitHub</a>.</blockquote><h3>Summary</h3><p>We’ve discussed quite a broad topic of causal inference, so let me recap what we’ve learned:</p><ul><li>The main goal of predictive analytics is to get accurate forecasts. The causal inference is focused on understanding the relationships, so we care more about the coefficients in the model than the actual predictions.</li><li>We can leverage linear regression to get the causal conclusions.</li><li>Understanding what features we should add to the linear regression is an art, but here is some guidance. <br> — You must include confounders (features that affect both treatment and outcome).<br> — Adding a feature that predicts the output variable and explains its variability can help you to get more confident estimations.<br> — Avoid adding features that either affect only treatment or are the outcome of the output variable.</li><li>You can use this approach for both A/B tests and observational data. However, with observations, we should treat our causal conclusions with a pinch of salt because we can never be sure that we accounted for all confounders.</li></ul><blockquote>Thank you a lot for reading this article. If you have any follow-up questions or comments, please leave them in the comments section.</blockquote><h3>Dataset</h3><p><em>Cortez, Paulo. (2014). Student Performance.</em> <em>UCI Machine Learning Repository (CC BY 4.0). </em><a href="https://doi.org/10.24432/C5TG7T.">https://doi.org/10.24432/C5TG7T</a></p><h3>Reference</h3><p><em>All the images are produced by the author unless otherwise stated.</em></p><p>This article is inspired by the book <a href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html">Causal Inference for the Brave and True</a> that gives a wonderful overview on the causal inference basics.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=34c6317c5a11" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/linear-regressions-for-causal-conclusions-34c6317c5a11">Linear Regressions for Causal Conclusions</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
            </content:encoded>
        </item>
    </channel>
</rss>